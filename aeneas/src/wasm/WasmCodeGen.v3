// Copyright 2017 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def XXX: Terminal;
class WasmCodeGen extends ArchCodeGen {
	def wasm: WasmProgram;
	def rt: MachRuntime;
	def stackgen = WasmStackInstrGen.new();
	def stackifier = MachStackifier.new(stackgen);
	def matcher = SsaInstrMatcher.new();
	def spiller = ShadowStackSpiller.new();
	def cfgr = CfgRestructurer.new();

	def MATCH_NEG = true;
	def MATCH_OP_I = true;

	new(context: SsaContext, wasm: WasmProgram, rt, buffer: MachDataBuffer) super(context, wasm.mach, ArmMachRegs.regs, buffer) {
		anyReg = ArmMachRegs.GPR;
		stackgen.context = context;
		stackifier.printInstr = printInstr;
	}
	def generateWasm(m: IrMethod) {
		context.enterMethod(m);
		var order = SsaBlockOrder.new(m.ssa);
		reset(m.ssa, order, null);
		cfgr.context = context;
		var code = cfgr.gen(order);
		if (Aeneas.PRINT_MACH.get()) cfgr.print(context, regSet, code);
		// Reset locals and assign parameter spill slots.
		stackgen.reset(m, m.ssa.params, getVreg);
		// Select instructions for CFG code.
		blockEnd = cursor = ArchInstr.new(ArchInstrs.ARCH_END, []);
		for (i = code.length - 1; i >= 0; i--) {
			selectInstructionsForCfgInstr(code[i]);
			advanceCursor();
		}
		spiller.reset(this);
		if (Aeneas.PRINT_MACH.get()) {
			print();
			spiller.print();
		}
		// Assemble WASM bytecode into the buffer.
		var sizepos = buffer.skip_u4leb();
		var start = buffer.pos;
		stackgen.emitVarDecls(buffer);
		assembleInstrs();
		var size = buffer.pos - start;
		buffer.overwrite_u4leb(sizepos, size);
		cursor = null;
	}
	def selectInstructionsForCfgInstr(c: CfgInstr) {
		match (c) {
			Unreachable => {
				emit0(WasmOp.UNREACHABLE.opcode);
			}
			Fallthrough(bi, vals) => {
				context.block = bi.block;
				for (v in vals) use(v);
				emitN(ArchInstrs.ARCH_NOP);
			}
			Ret(r) => {
				context.block = r.block();
				for (e in r.inputs) use(e.dest);
				emitN(ArchInstrs.ARCH_RET);
			}
			Block(t, bi) => {
				blockEnd = cursor;
				emit1(WasmOp.BLOCK.opcode, useInt(t));
			}
			Loop(bi) => {
				emit1(WasmOp.LOOP.opcode, useInt(WasmType.Void));
				blockEnd = cursor;
			}
			Body(bi) => {
				var block = bi.block, prevEnd = blockEnd;
				context.block = block;
				gatherLivenessForBlock(block);
				selectInstructionsForBlock(block);
				stackifier.stackify(block.uid, cursor, prevEnd, false);
				blockEnd = cursor;
			}
			If(split, val, t, join) => {
				context.block = split;
				blockEnd = cursor;
				emit2(WasmOp.IF.opcode, useInt(t), use(val));
			}
			Else => {
				emit0(WasmOp.ELSE.opcode);
				blockEnd = cursor;
			}
			End => {
				blockEnd = cursor;
				emit0(WasmOp.END.opcode);
			}
			Br(bi, reldepth) => {
				blockEnd = cursor;
				emit1(WasmOp.BR.opcode, useInt(reldepth));
			}
			BrIf(split, val, bi, reldepth) => {
				context.block = split;
				blockEnd = cursor;
				emit2(WasmOp.BR_IF.opcode, use(val), useInt(reldepth));
			}
			BrTable(split, val, min, reldepths) => {
				context.block = split;
				blockEnd = cursor;
				if (min != 0) {
					var t = newTmpVreg(WasmType.I32);
					emit3(WasmOp.I32_SUB.opcode, dfnv(t), use(val), use(context.graph.intConst(min)));
					usev(t);
				} else {
					use(val);
				}
				for (i < reldepths.length) useImm(Int.box(reldepths[i]));
				emitN(WasmOp.BR_TABLE.opcode);
			}
		}
	}
	def printCfgCode(code: Vector<CfgInstr>) {
		var out = this.out;
		// print header
		out.puts("Wasm CFG instructions for: ");
		var render: StringBuffer -> StringBuffer;
		if (context.spec != null) context.spec.render(out);
		else if (context.method != null) context.method.renderLong(out);
		out.outln();

		var depth = 0;
		for (i < code.length) {
			var c = code[i];
			depth += cfgr.render(out, c);
			out.outln();
		}
	}
	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => emitIntBinop(i, WasmOp.I32_ADD, WasmOp.I64_ADD);
			IntSub => emitIntBinop(i, WasmOp.I32_SUB, WasmOp.I64_SUB);
			IntMul => emitIntBinop(i, WasmOp.I32_MUL, WasmOp.I64_MUL);
			IntDiv => emitIntBinopSU(i, WasmOp.I32_DIV_S, WasmOp.I32_DIV_U, WasmOp.I64_DIV_S, WasmOp.I64_DIV_U);
			IntMod => emitIntBinopSU(i, WasmOp.I32_REM_S, WasmOp.I32_REM_U, WasmOp.I64_REM_S, WasmOp.I64_REM_U);
			IntAnd => emitIntBinop(i, WasmOp.I32_AND, WasmOp.I64_AND);
			IntOr  => emitIntBinop(i, WasmOp.I32_OR, WasmOp.I64_OR);
			IntXor => emitIntBinop(i, WasmOp.I32_XOR, WasmOp.I64_XOR);
			IntShl => emitShift(i, WasmOp.I32_SHL, WasmOp.I64_SHL);
			IntSar => emitShift(i, WasmOp.I32_SHR_S, WasmOp.I64_SHR_S);
			IntShr => emitShift(i, WasmOp.I32_SHR_U, WasmOp.I64_SHR_U);
			IntConvert => emitIntConvert(i);
			BoolNot => {
				// XXX: match and invert comparisons
				emit3(WasmOp.I32_XOR.opcode, dfn(i), use(i.input0()), useIntConst(1));
			}
			BoolAnd => emit3(WasmOp.I32_AND.opcode, dfn(i), use(i.input0()), use(i.input1()));
			BoolOr => emit3(WasmOp.I32_OR.opcode, dfn(i), use(i.input0()), use(i.input1()));
			BoolEq,
			RefEq,
			IntEq	=> emitEq(i);
			IntLt	=> emitIntBinopSU(i, WasmOp.I32_LT_S, WasmOp.I32_LT_U, WasmOp.I64_LT_S, WasmOp.I64_LT_U);
			IntLteq => emitIntBinopSU(i, WasmOp.I32_LE_S, WasmOp.I32_LE_U, WasmOp.I64_LE_S, WasmOp.I64_LE_U);
			UnsignedLt => emitBinop(i, WasmOp.I32_LT_U);
			UnsignedLteq => emitBinop(i, WasmOp.I32_LE_U);
			UnsignedGt => emitBinop(i, WasmOp.I32_GT_U);
			UnsignedGteq => emitBinop(i, WasmOp.I32_GE_U);
			PtrLoad => {
				var ty = i.op.typeArgs[1];
				var wop: WasmOp, size = mach.sizeOf(ty);
				match (size) {
					1 => wop = if(mach.isSigned(ty), WasmOp.I32_LOAD8_S, WasmOp.I32_LOAD8_U);
					2 => wop = if(mach.isSigned(ty), WasmOp.I32_LOAD16_S, WasmOp.I32_LOAD16_U);
					4 => wop = WasmOp.I32_LOAD;
					8 => wop = WasmOp.I64_LOAD;
					_ => context.fail("invalid load size");
				}
				var t = matchAddress(i.input0()), opcode = wop.opcode | ArchInstrs.FLAG_LOAD;
				if (t.1 == null) {
					emit3(opcode, dfn(i), useImm(t.0), use(context.graph.zeroConst()));
				} else {
					emit3(opcode, dfn(i), useImm(t.0), use(t.1));
				}
			}
			PtrStore => {
				var wop: WasmOp, size = mach.sizeOf(i.op.typeArgs[1]);
				var l = isLong(i.input1().getType());  // handle store narrowing
				match (size) {
					1 => wop = if(l, WasmOp.I64_STORE8, WasmOp.I32_STORE8);
					2 => wop = if(l, WasmOp.I64_STORE16, WasmOp.I32_STORE16);
					4 => wop = if(l, WasmOp.I64_STORE32, WasmOp.I32_STORE);
					8 => wop = WasmOp.I64_STORE;
					_ => context.fail("invalid store size");
				}
				var t = matchAddress(i.input0()), opcode = wop.opcode | ArchInstrs.FLAG_STORE;
				if (t.1 == null) {
					emit3(opcode, useImm(t.0), use(context.graph.zeroConst()), use(i.input1()));
				} else {
					emit3(opcode, useImm(t.0), use(t.1), use(i.input1()));
				}
			}
			PtrAdd => emitBinop(i, WasmOp.I32_ADD);
			PtrSub => emitBinop(i, WasmOp.I32_SUB);
			ConditionalThrow => {
				emit2(WasmOp.IF.opcode, useInt(WasmType.Void), use(i.inputs[0].dest));
				emitN(WasmOp.UNREACHABLE.opcode);
				emitN(WasmOp.END.opcode);
			}
			CallAddress => {
				dfnAll(i);
				kill(newLivepoint(), 0);
				var target = i.inputs[0].dest;
				if (SsaConst.?(target)) {
					var val = SsaConst.!(target).val, m: IrMethod;
					if (val == null) {  // call of a null address => always throws
						emitN(WasmOp.UNREACHABLE.opcode);
						return;
					}
					if (FuncVal.?(val)) m = FuncVal.!(val).memberRef.asMethod();
					else if (Address<IrMethod>.?(val)) m = Address<IrMethod>.!(val).val;
					else context.fail("constant target is not an address or function value");
					var start = if(V3.isComponent(m.receiver), 2, 1);
					useAll(i.inputs, start); // skip target and maybe receiver
					useInt(m.machIndex);
					emitN(WasmOp.CALL.opcode);
				} else {
					var sig = FuncType.!(i.op.typeArgs[0].nested.head).sig();
					var sigIndex = wasm.addSig(Void.TYPE, sig);
					useAll(i.inputs, 1);   // use arguments, except target
					use(i.inputs[0].dest); // target of the call comes last
					useInt(int.!(sigIndex));
					emitN(WasmOp.CALL_INDIRECT.opcode);
					wasm.containsCallIndirect = true;
				}
				// XXX: match Call(PtrLoad(PtrAdd(#meta, x))
			}
			Alloc => {
				emit3(WasmOp.CALL.opcode, dfn(i), use(i.inputs[0].dest), useInt(wasm.allocateStubFuncIndex));
			}
			MachSystemOp => context.fail("unimplemented MachSystemOp");
			_ => return context.fail1("unexpected opcode %1 in WASM codegen", i.op.opcode.name);
		}
	}
	def emitEq(i: SsaApplyOp) {
		var l = i.op.typeArgs.length > 0 && isLong(i.op.typeArgs[0]);
		matcher.binop(i);
		if (matcher.yzero) {
			var op = if (l, WasmOp.I64_EQZ, WasmOp.I32_EQZ);
			emit2(op.opcode, dfn(i), use(matcher.x));
		} else {
			var op = if(l, WasmOp.I64_EQ, WasmOp.I32_EQ);
			emit3(op.opcode, dfn(i), use(matcher.x), use(matcher.y));
		}
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {
		emitN(WasmOp.UNREACHABLE.opcode); // TODO: record exception location
	}
	def visitIf(block: SsaBlock, i: SsaIf) {
		// do nothing; handled by CfgRestructurer
	}
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {
		// do nothing; handled by CfgRestructurer
	}
	def visitGoto(block: SsaBlock, target: SsaGoto) {
		// do nothing; handled by CfgRestructurer
	}
	def visitReturn(block: SsaBlock, i: SsaReturn) {
		// do nothing; handled by CfgRestructurer
	}
	def emitIntConvert(i: SsaApplyOp) {
		var ft = IntType.!(i.op.sig.paramTypes[0]), tt = IntType.!(i.op.sig.returnType());
		if (ft.width > 32) {
			// Converting from 64-bit
			if (tt.width > 32) {
				// long-long conversion.
				if (tt.signed) {
					var t = newTmpVreg(WasmType.I64);
					emit3(WasmOp.I64_SHL.opcode, dfnv(t), use(i.input0()), useLongConst(64 - tt.width));
					emit3(WasmOp.I64_SHR_S.opcode, dfn(i), usev(t), useLongConst(64 - tt.width));
				} else {
					emit3(WasmOp.I64_AND.opcode, dfn(i), use(i.input0()), useConstant(context.graph.valConst(Long.TYPE, tt.max), 0));
				}
			} else if (tt.width == 32) {
				// long-int conversion.
				emit2(WasmOp.I32_WRAP_I64.opcode, dfn(i), use(i.input0()));
			} else {
				// long-sub-int conversion.
				var t1 = newTmpVreg(WasmType.I32);
				emit2(WasmOp.I32_WRAP_I64.opcode, dfnv(t1), use(i.input0()));
				if (tt.signed) {
					var t2 = newTmpVreg(WasmType.I32);
					emit3(WasmOp.I32_SHL.opcode, dfnv(t2), usev(t1), useIntConst(32 - tt.width));
					emit3(WasmOp.I32_SHR_S.opcode, dfn(i), usev(t2), useIntConst(32 - tt.width));
				} else {
					emit3(WasmOp.I32_AND.opcode, dfn(i), usev(t1), useConstant(context.graph.valConst(Int.TYPE, tt.max), 0));
				}
			}
		} else {
			// Converting from 32-bit
			if (tt.width <= 32) {
				// int-int conversion.
				if (tt.signed) {
					var t = newTmpVreg(WasmType.I32);
					emit3(WasmOp.I32_SHL.opcode, dfnv(t), use(i.input0()), useIntConst(32 - tt.width));
					emit3(WasmOp.I32_SHR_S.opcode, dfn(i), usev(t), useIntConst(32 - tt.width));
				} else {
					emit3(WasmOp.I32_AND.opcode, dfn(i), use(i.input0()), useConstant(context.graph.valConst(Int.TYPE, tt.max), 0));
				}
			} else {
				// int-long conversion.
				var op = if(ft.signed, WasmOp.I64_EXTEND_S_I32, WasmOp.I64_EXTEND_U_I32);
				emit2(op.opcode, dfn(i), use(i.input0()));
			}
		}
	}
	def matchAddress(a: SsaInstr) -> (Val, SsaInstr) {
		if (SsaConst.?(a)) return (SsaConst.!(a).val, null);
		var add = cover(Opcode.PtrAdd, a);
		if (add != null) {
			var r = add.input1();
			if (SsaConst.?(r)) return (SsaConst.!(r).val, add.input0());
		}
		return (null, a);
	}
	def emitShift(i: SsaApplyOp, op32: WasmOp, op64: WasmOp) {
		if (IntType.!(i.op.typeArgs[0]).width <= 32) {
			emit3(op32.opcode, dfn(i), use(i.input0()), use(i.input1()));
		} else {
			// wasm requires a 64-bit shiftend
			var t = newTmpVreg(WasmType.I64);
			emit2(WasmOp.I64_EXTEND_U_I32.opcode, dfnv(t), use(i.input1()));
			emit3(op64.opcode, dfn(i), use(i.input0()), usev(t));
		}
	}
	def emitIntBinop(i: SsaApplyOp, op32: WasmOp, op64: WasmOp) {
		var op = if(isLong(i.op.typeArgs[0]), op64, op32);
		emit3(op.opcode, dfn(i), use(i.input0()), use(i.input1()));
	}
	def emitIntBinopSU(i: SsaApplyOp, op32s: WasmOp, op32u: WasmOp, op64s: WasmOp, op64u: WasmOp) {
		if (isSigned(i.op)) emitIntBinop(i, op32s, op64s);
		else emitIntBinop(i, op32u, op64u);
	}
	def emitBinop(i: SsaApplyOp, op: WasmOp) {
		emit3(op.opcode, dfn(i), use(i.input0()), use(i.input1()));
	}
	def newTmpVreg(wasmType: byte) -> VReg {
		return stackgen.allocSlot(newVreg(null), wasmType);
	}
	// Assembling support
	def assemble(opcode: int, a: Array<Operand>) {
		match (opcode) {
			ArchInstrs.ARCH_NOP => return;
			ArchInstrs.ARCH_RET => return buffer.i1(WasmOp.RETURN.opcode);
			ArchInstrs.ARCH_BLOCK => {
				context.block = toBlock(a[0]);
				return;
			}
			ArchInstrs.ARCH_END => return buffer.i1(WasmOp.END.opcode);
			ArchInstrs.ARCH_PARMOVE => return context.fail("parallel move instructions should be removed by stackifier");
		}
		if (opcode >= 0 && WasmOpNames.array[opcode] != null) buffer.i1(opcode);
		else context.fail1("cannot assemble WASM opcode %1", opcode);
		// emit immediates for specific opcodes
		match (opcode) {
			WasmOp.GET_LOCAL.opcode,
			WasmOp.SET_LOCAL.opcode,
			WasmOp.TEE_LOCAL.opcode => {
				var v = toVar(a[0]), slot = stackgen.slotOf(v);
				if (slot < 0) context.fail1("variable @%1 not allocated a spill slot", v.ssa.uid);
				buffer.i4leb(slot);
			}

			WasmOp.BR.opcode,
			WasmOp.LOOP.opcode,
			WasmOp.BLOCK.opcode,
			WasmOp.IF.opcode => buffer.i1(toInt(a[0]));

			WasmOp.BR_IF.opcode => buffer.u4leb(u32.!(toInt(a[1])));
			WasmOp.BR_TABLE.opcode => {
				var count = a.length - 2;
				buffer.u4leb(u32.!(count));
				for (i = 1; i < a.length; i++) {
					buffer.u4leb(u32.!(toInt(a[i])));
				}
			}

			WasmOp.I32_LOAD8_S.opcode,
			WasmOp.I32_LOAD8_U.opcode,
			WasmOp.I64_LOAD8_S.opcode,
			WasmOp.I64_LOAD8_U.opcode => asmls(0, a, 1);

			WasmOp.I32_STORE8.opcode,
			WasmOp.I64_STORE8.opcode => asmls(0, a, 0);

			WasmOp.I32_LOAD16_S.opcode,
			WasmOp.I32_LOAD16_U.opcode,
			WasmOp.I64_LOAD16_S.opcode,
			WasmOp.I64_LOAD16_U.opcode => asmls(1, a, 1);

			WasmOp.I32_STORE16.opcode,
			WasmOp.I64_STORE16.opcode => asmls(1, a, 0);

			WasmOp.I32_LOAD.opcode,
			WasmOp.F32_LOAD.opcode => asmls(2, a, 1);

			WasmOp.I64_LOAD32_S.opcode,
			WasmOp.I64_LOAD32_U.opcode => asmls(3, a, 1);

			WasmOp.I32_STORE.opcode,
			WasmOp.F32_STORE.opcode => asmls(2, a, 0);

			WasmOp.I64_STORE32.opcode => asmls(2, a, 0);

			WasmOp.I64_LOAD.opcode,
			WasmOp.F64_LOAD.opcode => asmls(3, a, 1);

			WasmOp.I64_STORE.opcode,
			WasmOp.F64_STORE.opcode => asmls(3, a, 0);

			WasmOp.I64_CONST.opcode,
			WasmOp.I32_CONST.opcode => emitInt(a[0]);
			WasmOp.CALL.opcode => {
				emitInt(a[a.length - 1]); // function_index
			}
			WasmOp.CALL_INDIRECT.opcode => {
				emitInt(a[a.length - 1]); // type_index
				buffer.i1(0);	 // reserved
			}
		}
	}
	def emitAddr(addr: Addr) {
		buffer.recordPatch(addr, buffer.pos);
		buffer.skipN(5);
	}
	def emitInt(o: Operand) {
		var val = Operand.Immediate.!(o).val;
		if (val == null) {
			return buffer.i1(0);
		} else if (Addr.?(val)) {
			emitAddr(Addr.!(val));
		} else if (Box<int>.?(val)) {
			buffer.i4leb(Box<int>.!(val).val);
		} else if (Box<long>.?(val)) {
			buffer.i8leb(Box<long>.!(val).val);
		} else {
			context.fail1("cannot convert immediate to int: %1", V3.renderVal(val));
		}
	}
	def asmls(logAlign: int, a: Array<Operand>, offsetOperand: int) {
		buffer.i1(logAlign);	// flags = alignment
		emitInt(a[offsetOperand]);  // offset
	}
	// Printing support
	def renderSpecific(opcode: int, a: Array<Operand>) -> bool {
		var name = WasmOpNames.array[opcode];
		match (opcode) {
			WasmOp.IF.opcode,
			WasmOp.BLOCK.opcode,
			WasmOp.LOOP.opcode => {
				out.putc('\t').puts(name);
				out.puts(": ");
				var typ = WasmType.name(byte.!(toInt(a[0])));
				out.yellow().puts(typ).end();
				if (a.length > 1) out.sp().putOperand(a[1]);
				return true;
			}
			WasmOp.BR.opcode => {
				out.putc('\t').puts(name);
				out.puts(" depth=");
				out.green().puti(toInt(a[0])).end();
				return true;
			}
			WasmOp.BR_IF.opcode => {
				out.putc('\t').puts(name);
				out.sp();
				out.putOperand(a[0]);
				out.puts(" depth=");
				out.green().puti(toInt(a[1])).end();
				return true;
			}
			WasmOp.I32_LOAD.opcode,
			WasmOp.I64_LOAD.opcode,
			WasmOp.F32_LOAD.opcode,
			WasmOp.F64_LOAD.opcode,
			WasmOp.I32_LOAD8_S.opcode,
			WasmOp.I32_LOAD8_U.opcode,
			WasmOp.I32_LOAD16_S.opcode,
			WasmOp.I32_LOAD16_U.opcode,
			WasmOp.I64_LOAD8_S.opcode,
			WasmOp.I64_LOAD8_U.opcode,
			WasmOp.I64_LOAD16_S.opcode,
			WasmOp.I64_LOAD16_U.opcode,
			WasmOp.I64_LOAD32_S.opcode,
			WasmOp.I64_LOAD32_U.opcode => {
				out.putc('\t').puts(name);
				out.puts(" offset=").green();
				V3.renderResult(toImm(a[1]), null, out);
				out.end().sp();
				if (Operand.Def.?(a[0])) out.putOperand(a[0]).sp();
				if (a.length > 2) out.putOperand(a[2]);
				return true;
			}
			WasmOp.I32_STORE.opcode,
			WasmOp.I64_STORE.opcode,
			WasmOp.F32_STORE.opcode,
			WasmOp.F64_STORE.opcode,
			WasmOp.I32_STORE8.opcode,
			WasmOp.I32_STORE16.opcode,
			WasmOp.I64_STORE8.opcode,
			WasmOp.I64_STORE16.opcode,
			WasmOp.I64_STORE32.opcode => {
				out.putc('\t').puts(name);
				out.puts(" offset=").green();
				V3.renderResult(toImm(a[0]), null, out);
				out.end().sp();
				for (j = 1; j < a.length; j++) {
					if (j > 1) out.puts(", ");
					out.putOperand(a[j]);
				}
				return true;
			}
			_ => {
				if (name == null) return false;
				out.putArch(name, a);
				return true;
			}
		}
	}
	def generateEntryStub() {
		def b = buffer.i1;
		var sizepos = buffer.pos;

		b(/*body_size=*/0);
		var start = buffer.pos;
		b(/*locals_count=*/0);

		// ============ entrypoint stub code =========================
		var m = rt.getRiInit();
		b(WasmOp.GET_LOCAL.opcode); b(0);
		b(WasmOp.CALL.opcode); buffer.i4leb(m.machIndex);
		var main = mach.prog.getMain().asMethod();
		if (main.sig.paramTypes.length == 0) {
			// main with no parameters => drop args
			b(WasmOp.DROP.opcode);
		}
		b(WasmOp.CALL.opcode); buffer.i4leb(main.machIndex);
		if (main.sig.returnTypes.length == 0) {
			// main with no return => emit i32.const #0
			b(WasmOp.I32_CONST.opcode);
			b(0);
		}
		b(WasmOp.END.opcode);
		// ============================================================
		var bodysize = buffer.pos - start;
		buffer.at(sizepos).i1(bodysize);
		buffer.atEnd();
	}
	def generateAllocationStub() {
		var gcmeth = rt.getRiGc();
		def b = buffer.i1;

		var sizepos = buffer.pos;
		b(/*body_size=*/0);
		var start = buffer.pos;

		def tSize = 0;
		def tNewHeapCur = 1;
		// A temporary local is needed to use the new heap cur twice
		b(/*locals_count=*/1);
		buffer.i4leb(1); buffer.i1(WasmType.I32);

		// ============ allocation stub code =========================
		genLoadRtAddr_i32(CiRuntimeModule.HEAP_CUR_LOC);
		b(WasmOp.I32_CONST.opcode); emitAddr(CiRuntimeModule.HEAP_CUR_LOC);
		genLoadRtAddr_i32(CiRuntimeModule.HEAP_CUR_LOC);
		b(WasmOp.GET_LOCAL.opcode); b(0);
		b(WasmOp.I32_ADD.opcode);
		b(WasmOp.TEE_LOCAL.opcode); b(tNewHeapCur);
		// if (newHeapCur > heapEnd)
		genLoadRtAddr_i32(CiRuntimeModule.HEAP_END_LOC);
		b(WasmOp.I32_GT_S.opcode);
		b(WasmOp.IF.opcode); b(WasmType.Void);
		if (gcmeth != null) {
			// return gcmeth(size);
			b(WasmOp.GET_LOCAL.opcode); b(tSize);
			b(WasmOp.CALL.opcode); buffer.i4leb(gcmeth.machIndex);
			b(WasmOp.RETURN.opcode);
		} else {
			// if no RiRuntime.gc() method, just crash.
			b(WasmOp.UNREACHABLE.opcode);
		}
		// end if
		b(WasmOp.END.opcode);
                b(WasmOp.GET_LOCAL.opcode); b(tNewHeapCur);
		b(WasmOp.I32_STORE.opcode); b(/*align=*/2); b(/*offset=*/0);
		b(WasmOp.END.opcode);
		// ============================================================
		var bodysize = buffer.pos - start;
		buffer.at(sizepos).i1(bodysize);
		buffer.atEnd();
	}
	def generateShadowStackAlloc(shadow_sp: int, count: int) {
		// Shadow stack grows up, until the end of memory, falling off the end.
		def b = buffer.i1;
		genLoadRtAddr_i32(CiRuntimeModule.SHADOW_STACK_CUR_PTR);
		genLoadRtAddr_i32(CiRuntimeModule.SHADOW_STACK_CUR_PTR);
		b(WasmOp.TEE_LOCAL.opcode); buffer.i4leb(shadow_sp);
		b(WasmOp.I32_CONST.opcode); buffer.i4leb(count);
		b(WasmOp.I32_ADD.opcode);
		b(WasmOp.I32_STORE.opcode); b(/*align=*/2); b(/*offset=*/0);
	}
	def generateShadowStackFree(shadow_sp: int, count: int) {
		def b = buffer.i1;
		genLoadRtAddr_i32(CiRuntimeModule.SHADOW_STACK_CUR_PTR);
		b(WasmOp.GET_LOCAL.opcode); buffer.i4leb(shadow_sp);
		b(WasmOp.I32_CONST.opcode); buffer.i4leb(count);
		b(WasmOp.I32_SUB.opcode);
		b(WasmOp.I32_STORE.opcode); b(/*align=*/2); b(/*offset=*/0);
	}
	def generateSpillToShadowStack(v: VReg, shadow_sp: int) {
		def b = buffer.i1;
		b(WasmOp.GET_LOCAL.opcode); buffer.i4leb(shadow_sp);
		b(WasmOp.GET_LOCAL.opcode); buffer.i4leb(stackgen.slotOf(v));
		b(WasmOp.I32_STORE.opcode); b(/*align=*/2); b(/*offset=*/v.spill * mach.data.addressSize);
	}
	def generateReloadFromShadowStack(v: VReg, shadow_sp: int) {
		def b = buffer.i1;
		b(WasmOp.GET_LOCAL.opcode); buffer.i4leb(shadow_sp);
		b(WasmOp.I32_LOAD.opcode); b(/*align=*/2); b(/*offset=*/v.spill * mach.data.addressSize);
	}
	def generateIndirectAdapter(sig: Signature, index: u32, m: IrMethod) {
		def b = buffer.i1;

		var sizepos = buffer.skip_u4leb(); /*body_size*/
		var start = buffer.pos;
		b(/*locals_count=*/0);

		// indirect adapters just drop the first parameter, so just load
		// all arguments except the first one and call the target
		// ============ indirect adapter code ========================
		for (i = 1; i < m.ssa.params.length; i++) {
			b(WasmOp.GET_LOCAL.opcode); buffer.i4leb(i);
		}
		b(WasmOp.CALL.opcode);
		buffer.i4leb(m.machIndex);
		b(WasmOp.END.opcode);
		// ===========================================================

		buffer.overwrite_u4leb(sizepos, buffer.pos - start);
		buffer.atEnd();
	}
	def genLoadRtAddr_i32(addr: CiRuntime_Address) {
		def b = buffer.i1;
		b(WasmOp.I32_CONST.opcode); emitAddr(addr);
		b(WasmOp.I32_LOAD.opcode); b(/*align=*/2); b(/*offset=*/0);
	}
}
def isLong(t: Type) -> bool {
	if (IntType.?(t)) return IntType.!(t).width > 32;
	if (V3EnumSet_TypeCon.?(t.typeCon)) return V3.getEnumSetType(t).byteSize() > 4;
	return false;
}
// A stack instruction generator specifically for WASM.
// Since it may dynamically assign locals for machine variables, it manages this mapping
// as well.
class WasmStackInstrGen extends StackInstrGen {
	var context: SsaContext;
	var params = 0;
	var i32_count = 0;
	var i64_count = 0;
	var f32_count = 0;
	var f64_count = 0;
	def reset(m: IrMethod, params: Array<SsaParam>, getVreg: SsaInstr -> VReg) {
		i32_count = 0;
		i64_count = 0;
		f32_count = 0;
		f64_count = 0;

		var slot = if(V3.isComponent(m.receiver), 0, 1);
		for (p in params) {
			var v = getVreg(p);
			v.spill = slot++;
			v.hint = WasmType.of(p.vtype);
		}
		this.params = slot - 1;
	}
	def mapVar(v: VReg) -> VReg {
		if (v.hint != 0) return v;
		return allocSlot(v, WasmType.of(v.ssa.getType()));
	}
	def allocSlot(v: VReg, wasmType: byte) -> VReg{
		var index = 0;
		match (v.hint = wasmType) {
			WasmType.I32 => index = ++i32_count;
			WasmType.I64 => index = ++i64_count;
			WasmType.F32 => index = ++f32_count;
			WasmType.F64 => index = ++i64_count;
			_ => return V3.fail1("invalid WasmType for variable @%1", v.ssa.uid);
		}
		v.spill = params + index;
		return v;
	}
	def slotOf(v: VReg) -> int {
		var slot = v.spill - 1;
		if (v.spill < 0) return -1;
		if (slot < params) return slot;
		if (v.hint == WasmType.I32) return slot;
		slot += i32_count;
		if (v.hint == WasmType.I64) return slot;
		slot += i64_count;
		if (v.hint == WasmType.F32) return slot;
		slot += f32_count;
		if (v.hint != WasmType.F64) return V3.fail1("invalid WasmType for variable @%1", v.ssa.uid);
		return slot;
	}
	def emitVarDecls(buffer: MachDataBuffer) {
		var count = 0;
		if (i32_count > 0) count++;
		if (i64_count > 0) count++;
		if (f32_count > 0) count++;
		if (f64_count > 0) count++;
		buffer.i1(count);
		if (i32_count > 0) { buffer.i4leb(i32_count); buffer.i1(WasmType.I32); }
		if (i64_count > 0) { buffer.i4leb(i64_count); buffer.i1(WasmType.I64); }
		if (f32_count > 0) { buffer.i4leb(f32_count); buffer.i1(WasmType.F32); }
		if (f64_count > 0) { buffer.i4leb(f64_count); buffer.i1(WasmType.F64); }
	}
	def insertLoadLocal(v: VReg, next: ArchInstr) {
		var u = Operand.Use(v.ssa, mapVar(v), 0);
		insertBefore(ArchInstr.new(WasmOp.GET_LOCAL.opcode, [u]), next);
	}
	def insertStoreLocal(v: VReg, pop: bool, next: ArchInstr) {
		var opcode = if(pop, WasmOp.SET_LOCAL.opcode, WasmOp.TEE_LOCAL.opcode);
		var d = Operand.Def(v.ssa, mapVar(v), 0);
		insertBefore(ArchInstr.new(opcode, [d]), next);
	}
	def insertPop(v: VReg, next: ArchInstr) {
		insertBefore(ArchInstr.new(WasmOp.DROP.opcode, []), next);
	}
	def insertLoadConst(t: Type, val: Val, next: ArchInstr) {
		match (t.typeCon.kind) {
			V3Kind.ENUM,
			V3Kind.ENUM_SET,
			V3Kind.INT => {
				var opcode = if(isLong(t), WasmOp.I64_CONST.opcode, WasmOp.I32_CONST.opcode);
				insertBefore(ArchInstr.new(opcode, [Operand.Immediate(val)]), next);
			}
			V3Kind.BOOL => {
				var truth = if(Values.equal(Bool.TRUE, val), Int.ONE);
				insertBefore(ArchInstr.new(WasmOp.I32_CONST.opcode, [Operand.Immediate(truth)]), next);
			}
			MachType.MACH_POINTER,
			V3Kind.FUNCREF,
			V3Kind.ANYFUNC,
			V3Kind.CLASS,
			V3Kind.ARRAY,
			V3Kind.VARIANT => {
				var opcode = WasmOp.I32_CONST.opcode;
				insertBefore(ArchInstr.new(opcode, [Operand.Immediate(val)]), next);
			}
			_ => context.fail1("unimplemented insertLoadConst() of type %1", t.render);
		}
	}
}
