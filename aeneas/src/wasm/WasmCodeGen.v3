// Copyright 2017 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def TRUNCATE = true;
def NO_TRUNCATE = false;
class WasmCodeGen extends ArchCodeGen {
	def wasm: WasmProgram;
	def buffer: MachDataBuffer;
	def rt: MachRuntime;
	def m = SsaInstrMatcher.new();

	def MATCH_NEG = true;
	def MATCH_OP_I = true;

	new(context: SsaContext, wasm: WasmProgram, rt) super(context, wasm.mach, ArmMachRegs.regs) {
		anyReg = ArmMachRegs.GPR;
	}
	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => emitIntBinop(i, WasmOp.I32_ADD, WasmOp.I64_ADD);
			IntSub => emitIntBinop(i, WasmOp.I32_SUB, WasmOp.I64_SUB);
			IntMul => emitIntBinop(i, WasmOp.I32_MUL, WasmOp.I64_MUL);
			IntDiv => emitIntBinopSU(i, WasmOp.I32_DIV_S, WasmOp.I32_DIV_U, WasmOp.I64_DIV_S, WasmOp.I64_DIV_U);
			IntMod => emitIntBinopSU(i, WasmOp.I32_REM_S, WasmOp.I32_REM_U, WasmOp.I64_REM_S, WasmOp.I64_REM_U);
			IntAnd => emitIntBinop(i, WasmOp.I32_AND, WasmOp.I64_AND);
			IntOr  => emitIntBinop(i, WasmOp.I32_OR, WasmOp.I64_OR);
			IntXor => emitIntBinop(i, WasmOp.I32_XOR, WasmOp.I64_XOR);
			IntShl => emitIntBinop(i, WasmOp.I32_SHL, WasmOp.I64_SHL);
			IntSar => emitIntBinop(i, WasmOp.I32_SHR_S, WasmOp.I64_SHR_S);
			IntShr => emitIntBinop(i, WasmOp.I32_SHR_U, WasmOp.I64_SHR_U);
			IntConvert => emitIntConvert(i);
			BoolNot => {
				// XXX: match and invert comparisons
				emit3(archOp(WasmOp.I32_XOR), dfn(i), use(i.input0()), useInt(1));
			}
			BoolEq => emitBinop(i, WasmOp.I32_EQ);
			RefEq  => emitBinop(i, WasmOp.I32_EQ);
			IntEq   => emitIntBinop(i, WasmOp.I32_EQ, WasmOp.I64_EQ);
			IntLt   => emitIntBinopSU(i, WasmOp.I32_LT_S, WasmOp.I32_LT_U, WasmOp.I64_LT_S, WasmOp.I64_LT_U);
			IntLteq => emitIntBinopSU(i, WasmOp.I32_LE_S, WasmOp.I32_LE_U, WasmOp.I64_LE_S, WasmOp.I64_LE_U);
			UnsignedLt => emitBinop(i, WasmOp.I32_LT_U);
			UnsignedLteq => emitBinop(i, WasmOp.I32_LE_U);
			UnsignedGt => emitBinop(i, WasmOp.I32_GT_U);
			UnsignedGteq => emitBinop(i, WasmOp.I32_GE_U);
			PtrLoad => {
				var ty = i.op.typeArgs[1];
				var wop: WasmOp, align = mach.sizeOf(ty);
				match (align) {
					1 => wop = if(mach.isSigned(ty), WasmOp.I32_LOAD8_S, WasmOp.I32_LOAD8_U);
					2 => wop = if(mach.isSigned(ty), WasmOp.I32_LOAD16_S, WasmOp.I32_LOAD16_U);
					4 => wop = WasmOp.I32_LOAD;
					8 => wop = WasmOp.I64_LOAD;
					_ => context.fail("invalid load size"); // TODO: zero size loads
				}
				var t = matchAddress(i.input0()), opcode = archOp(wop) | ArchInstrs.FLAG_LOAD;
				if (t.1 == null) {
					emit3(opcode, dfn(i), useImm(t.0), useInt(0));
				} else {
					emit3(opcode, dfn(i), useImm(t.0), use(t.1));
				}
			}
			PtrStore => {
				var wop: WasmOp, align = mach.sizeOf(i.op.typeArgs[1]);
				match (align) {
					1 => wop = WasmOp.I32_STORE8;
					2 => wop = WasmOp.I32_STORE16;
					4 => wop = WasmOp.I32_STORE;
					8 => wop = WasmOp.I64_STORE;
					_ => context.fail("invalid store size");  // TODO: zero size stores
				}
				var t = matchAddress(i.input0()), opcode = archOp(wop) | ArchInstrs.FLAG_STORE;
				if (t.1 == null) {
					emit3(opcode, useImm(t.0), useInt(0), use(i.input1()));
				} else {
					emit3(opcode, useImm(t.0), use(t.1), use(i.input1()));
				}
			}
			PtrAdd => emitBinop(i, WasmOp.I32_ADD);
			PtrSub => emitBinop(i, WasmOp.I32_SUB);
			ConditionalThrow => ; // TODO
			CallAddress => ; // TODO
			Alloc => ;      // TODO
			MachSystemOp => ; // TODO
			_ => return context.fail("unexpected opcode in WASM codegen");
		}
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {} // TODO
	def visitIf(block: SsaBlock, i: SsaIf) {} // TODO
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {} // TODO
	def visitGoto(block: SsaBlock, target: SsaGoto) {} // TODO
	def visitReturn(block: SsaBlock, i: SsaReturn) {
		for (j < i.inputs.length) use(i.inputs[j].dest);
		emitN(ArchInstrs.ARCH_RET);
	}
	def emitIntConvert(i: SsaApplyOp) {
		var ft = IntType.!(i.op.paramTypes[0]), tt = IntType.!(i.op.resultType);
		if (ft.width > 32 && tt.width <= 32) {
			ft = Int.TYPE;
			emit2(archOp(WasmOp.I32_WRAP_I64), dfn(i), use(i.input0()));
		} else if (ft.width <= 32 && tt.width > 32) {
			var op = if(tt.signed, WasmOp.I64_EXTEND_S_I32, WasmOp.I64_EXTEND_U_I32);
			emit2(archOp(op), dfn(i), use(i.input0()));
		}
		if (tt.width != 32) {
			if (tt.signed) emitSignExtend(i, tt);
			else emitZeroExtend(i, tt);
		}
	}
	def emitSignExtend(i: SsaInstr, tt: IntType) {
		if (tt.width < 32) {
			emit3(archOp(WasmOp.I32_SHL), dfn(i), use(i), useInt(32 - tt.width));
			emit3(archOp(WasmOp.I32_SHR_S), dfn(i), use(i), useInt(32 - tt.width));
		} else if (tt.width > 32 && tt.width < 64) {
			emit3(archOp(WasmOp.I64_SHL), dfn(i), use(i), useInt(32 - tt.width));
			emit3(archOp(WasmOp.I64_SHR_S), dfn(i), use(i), useInt(32 - tt.width));
		}
	}
	def emitZeroExtend(i: SsaInstr, tt: IntType) {
		if (tt.width < 32) {
			emit3(archOp(WasmOp.I32_AND), dfn(i), use(i), useImm(tt.max));
		} else if (tt.width > 32 && tt.width < 64) {
			emit3(archOp(WasmOp.I64_AND), dfn(i), use(i), useImm(tt.max));
		}
	}
	def matchAddress(a: SsaInstr) -> (Val, SsaInstr) {
		if (SsaConst.?(a)) return (SsaConst.!(a).val, null);
		var add = cover(Opcode.PtrAdd, a);
		if (add != null) {
			var r = add.input1();
			if (SsaConst.?(r)) return (SsaConst.!(r).val, add.input0());
		}
		return (null, a);
	}
	def emitIntBinop(i: SsaApplyOp, op32: WasmOp, op64: WasmOp) {
		var op = if(IntType.!(i.op.typeArgs[0]).width <= 32, op32, op64);
		emit3(archOp(op), dfn(i), use(i.input0()), use(i.input1()));
	}
	def emitIntBinopSU(i: SsaApplyOp, op32s: WasmOp, op32u: WasmOp, op64s: WasmOp, op64u: WasmOp) {
		if (isSigned(i.op)) emitIntBinop(i, op32s, op64s);
		else emitIntBinop(i, op32u, op64u);
	}
	def emitBinop(i: SsaApplyOp, op: WasmOp) {
		emit3(archOp(op), dfn(i), use(i.input0()), use(i.input1()));
	}
	def archOp(op: WasmOp) -> int {
		return op.opcode + ArchInstrs.ARCH_COUNT;
	}
}
// Constants for wasm opcodes.
enum WasmOp(opcode: byte) {
	UNREACHABLE(0x00),
	NOP(0x01),
	BLOCK(0x02),
	LOOP(0x03),
	IF(0x04),
	ELSE(0x05),
	END(0x0b),
	BR(0x0c),
	BR_IF(0x0d),
	BR_TABLE(0x0e),
	RETURN(0x0f),
	CALL(0x10),
	CALL_INDIRECT(0x11),
	DROP(0x1a),
	SELECT(0x1b),
	GET_LOCAL(0x20),
	SET_LOCAL(0x21),
	TEE_LOCAL(0x22),
	GET_GLOBAL(0x23),
	SET_GLOBAL(0x24),
	I32_LOAD(0x28),
	I64_LOAD(0x29),
	F32_LOAD(0x2a),
	F64_LOAD(0x2b),
	I32_LOAD8_S(0x2c),
	I32_LOAD8_U(0x2d),
	I32_LOAD16_S(0x2e),
	I32_LOAD16_U(0x2f),
	I64_LOAD8_S(0x30),
	I64_LOAD8_U(0x31),
	I64_LOAD16_S(0x32),
	I64_LOAD16_U(0x33),
	I64_LOAD32_S(0x34),
	I64_LOAD32_U(0x35),
	I32_STORE(0x36),
	I64_STORE(0x37),
	F32_STORE(0x38),
	F64_STORE(0x39),
	I32_STORE8(0x3a),
	I32_STORE16(0x3b),
	I64_STORE8(0x3c),
	I64_STORE16(0x3d),
	I64_STORE32(0x3e),
	CURRENT_MEMORY(0x3f),
	GROW_MEMORY(0x40),
	I32_CONST(0x41),
	I64_CONST(0x42),
	F32_CONST(0x43),
	F64_CONST(0x44),
	I32_EQZ(0x45),
	I32_EQ(0x46),
	I32_NE(0x47),
	I32_LT_S(0x48),
	I32_LT_U(0x49),
	I32_GT_S(0x4a),
	I32_GT_U(0x4b),
	I32_LE_S(0x4c),
	I32_LE_U(0x4d),
	I32_GE_S(0x4e),
	I32_GE_U(0x4f),
	I64_EQZ(0x50),
	I64_EQ(0x51),
	I64_NE(0x52),
	I64_LT_S(0x53),
	I64_LT_U(0x54),
	I64_GT_S(0x55),
	I64_GT_U(0x56),
	I64_LE_S(0x57),
	I64_LE_U(0x58),
	I64_GE_S(0x59),
	I64_GE_U(0x5a),
	F32_EQ(0x5b),
	F32_NE(0x5c),
	F32_LT(0x5d),
	F32_GT(0x5e),
	F32_LE(0x5f),
	F32_GE(0x60),
	F64_EQ(0x61),
	F64_NE(0x62),
	F64_LT(0x63),
	F64_GT(0x64),
	F64_LE(0x65),
	F64_GE(0x66),
	I32_CLZ(0x67),
	I32_CTZ(0x68),
	I32_POPCNT(0x69),
	I32_ADD(0x6a),
	I32_SUB(0x6b),
	I32_MUL(0x6c),
	I32_DIV_S(0x6d),
	I32_DIV_U(0x6e),
	I32_REM_S(0x6f),
	I32_REM_U(0x70),
	I32_AND(0x71),
	I32_OR(0x72),
	I32_XOR(0x73),
	I32_SHL(0x74),
	I32_SHR_S(0x75),
	I32_SHR_U(0x76),
	I32_ROTL(0x77),
	I32_ROTR(0x78),
	I64_CLZ(0x79),
	I64_CTZ(0x7a),
	I64_POPCNT(0x7b),
	I64_ADD(0x7c),
	I64_SUB(0x7d),
	I64_MUL(0x7e),
	I64_DIV_S(0x7f),
	I64_DIV_U(0x80),
	I64_REM_S(0x81),
	I64_REM_U(0x82),
	I64_AND(0x83),
	I64_OR(0x84),
	I64_XOR(0x85),
	I64_SHL(0x86),
	I64_SHR_S(0x87),
	I64_SHR_U(0x88),
	I64_ROTL(0x89),
	I64_ROTR(0x8a),
	F32_ABS(0x8b),
	F32_NEG(0x8c),
	F32_CEIL(0x8d),
	F32_FLOOR(0x8e),
	F32_TRUNC(0x8f),
	F32_NEAREST(0x90),
	F32_SQRT(0x91),
	F32_ADD(0x92),
	F32_SUB(0x93),
	F32_MUL(0x94),
	F32_DIV(0x95),
	F32_MIN(0x96),
	F32_MAX(0x97),
	F32_COPYSIGN(0x98),
	F64_ABS(0x99),
	F64_NEG(0x9a),
	F64_CEIL(0x9b),
	F64_FLOOR(0x9c),
	F64_TRUNC(0x9d),
	F64_NEAREST(0x9e),
	F64_SQRT(0x9f),
	F64_ADD(0xa0),
	F64_SUB(0xa1),
	F64_MUL(0xa2),
	F64_DIV(0xa3),
	F64_MIN(0xa4),
	F64_MAX(0xa5),
	F64_COPYSIGN(0xa6),
	I32_WRAP_I64(0xa7),
	I32_TRUNC_S_F32(0xa8),
	I32_TRUNC_U_F32(0xa9),
	I32_TRUNC_S_F64(0xaa),
	I32_TRUNC_U_F64(0xab),
	I64_EXTEND_S_I32(0xac),
	I64_EXTEND_U_I32(0xad),
	I64_TRUNC_S_F32(0xae),
	I64_TRUNC_U_F32(0xaf),
	I64_TRUNC_S_F64(0xb0),
	I64_TRUNC_U_F64(0xb1),
	F32_CONVERT_S_I32(0xb2),
	F32_CONVERT_U_I32(0xb3),
	F32_CONVERT_S_I64(0xb4),
	F32_CONVERT_U_I64(0xb5),
	F32_DEMOTE_F64(0xb6),
	F64_CONVERT_S_I32(0xb7),
	F64_CONVERT_U_I32(0xb8),
	F64_CONVERT_S_I64(0xb9),
	F64_CONVERT_U_I64(0xba),
	F64_PROMOTE_F32(0xbb),
	I32_REINTERPRET_F32(0xbc),
	I64_REINTERPRET_F64(0xbd),
	F32_REINTERPRET_I32(0xbe),
	F64_REINTERPRET_I64(0xbf),
}
