// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def I_ADDD	= 0x01;		def I_ADDQ	= 0x11;
def I_ORD	= 0x02;		def I_ORQ	= 0x12;
def I_ADCD	= 0x03;		def I_ADCQ	= 0x13;
def I_ANDD	= 0x04;		def I_ANDQ	= 0x14;
def I_SUBD	= 0x05;		def I_SUBQ	= 0x15;
def I_XORD	= 0x06;		def I_XORQ	= 0x16;
def I_CMPD	= 0x07;		def I_CMPQ	= 0x17;
def I_MULD	= 0x08;		def I_MULQ	= 0x18;
def I_NEGD	= 0x09;		def I_NEGQ	= 0x19;
def I_NOTD	= 0x0A;		def I_NOTQ	= 0x1A;
def I_TESTD	= 0x0B;		def I_TESTQ	= 0x1B;
def I_LEAD	= 0x0C;		def I_LEAQ	= 0x1C;
def I_DIVD	= 0x0D;		def I_DIVQ	= 0x1D;
def I_IDIVD	= 0x0E;		def I_IDIVQ	= 0x1E;
def I_INCD	= 0x0F;		def I_INCQ	= 0x1F;

def I_DECD	= 0x20;		def I_DECQ	= 0x30;
def I_SHLD	= 0x21;		def I_SHLQ	= 0x31;
def I_SARD	= 0x22;		def I_SARQ	= 0x32;
def I_SHRD	= 0x23;		def I_SHRQ	= 0x33;
def I_CDQ	= 0x24;		def I_CQO	= 0x34;
def I_SWITCHD	= 0x25;		def I_SWITCHQ	= 0x35;
def I_ADDSS	= 0x26;		def I_ADDSD	= 0x36;
def I_SUBSS	= 0x27;		def I_SUBSD	= 0x37;
def I_MULSS	= 0x28;		def I_MULSD	= 0x38;
def I_DIVSS	= 0x29;		def I_DIVSD	= 0x39;
def I_SQRTSS	= 0x2A;		def I_SQRTSD	= 0x3A;
def I_MOVSS	= 0x2B;		def I_MOVSD	= 0x3B;
def I_CVTSS2SD	= 0x2C;		def I_CVTSD2SS	= 0x3C;
def I_PCMPEQD	= 0x2D;		def I_PCMPEQQ	= 0x3D;
def I_FLDD	= 0x2E;		def I_FLDQ	= 0x3E;
def I_UCOMISS	= 0x2F;		def I_UCOMISD	= 0x3F;

def I_CVTSS2SID	= 0x40;		def I_CVTSS2SIQ = 0x50;
def I_CVTSD2SID	= 0x41;		def I_CVTSD2SIQ = 0x51;
def I_CVTSI2SSD	= 0x42;		def I_CVTSI2SSQ = 0x52;
def I_CVTSI2SDD	= 0x43;		def I_CVTSI2SDQ = 0x53;
def I_ROUNDSS	= 0x45;		def I_ROUNDSD	= 0x55;
def I_PSLLD	= 0x46;		def I_PSLLQ	= 0x56;
def I_PSRLD	= 0x47;		def I_PSRLQ	= 0x57;

def I_MOVB		= 0x60;
def I_MOVBSX		= 0x61;
def I_MOVBZX		= 0x62;
def I_MOVW		= 0x63;
def I_MOVWSX		= 0x64;
def I_MOVWZX		= 0x65;
def I_JMP		= 0x66;
def I_JC		= 0x67;
def I_SETC		= 0x68;
def I_CALL		= 0x69;
def I_CALLER_IP		= 0x6A;
def I_CALLER_SP		= 0x6B;
def I_TEST_ALLOC	= 0x6C;
def I_CMPB		= 0x6F;
def I_THROW		= 0x70;
def I_THROWC		= 0x71;
def I_CMPXCHG8		= 0x72;
def I_CMPXCHG16		= 0x73;
def I_CMPXCHG32		= 0x74;
def I_CMPXCHG64		= 0x75;
def I_MOVD		= 0x76;
def I_MOVQ		= 0x77;

def I_QD_DIFF = I_ADDQ - I_ADDD; // Used to compute 64-bit opcode from 32-bit opcode

// Addressing modes:
// REG = register, OP = register/stack, IMM = immediate
// MRRSD = [reg + reg * scale + disp]
def AM_NONE	 = 0x00;
def AM_REG_OP	 = 0x01;
def AM_ADDR_IMM  = 0x02;
def AM_ADDR_REG  = 0x03;
def AM_MRRSD_REG = 0x04;
def AM_MRRSD_IMM = 0x05;
def AM_REG_MRRSD = 0x06;
def AM_OP	 = 0x07;
def AM_OP_IMM	 = 0x08;
def AM_OP_REG	 = 0x09;
def AM_XMM_REG	 = 0x0A;
def AM_XMM_OP	 = 0x0B;
def AM_OP_XMM	 = 0x0C;
def AM_XMM_MRRSD = 0x0D;
def AM_MRRSD_XMM = 0x0E;
def AM_REG_XMM	 = 0x0F;
def AM_XMM_IMM	 = 0x10;

def AM_SHIFT = u5.view(8);
def COND_SHIFT = u5.view(13);
def ROUNDING_SHIFT = u5.view(17);

def ABS_MARKER = 0x11223344;
def REL_MARKER = 0xAABBCCDD;

def MATCH_NEG = true;
def MATCH_OP_I = true;
def MATCH_ADDR_ADD = true;
def MATCH_ADDR_ADD_IMM = true;
def MATCH_ADDR_SCALE = true;
def MATCH_SCALE = true;

def Regs: X86_64RegSet;
def Conds: X86_64Conds;

// Generates X86-64 machine code from SSA.
class SsaX86_64Gen extends SsaMachGen {
	def asm: X86_64MacroAssembler;
	def m = SsaInstrMatcher.new();
	var labelUses: List<(int, Label)>;

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter) super(context, mach, Regs.SET, w) {
	}

	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => {
				emitSimpleBinop(selectWidth(i, I_ADDD), i, m.intbinop(i));
			}
			IntSub => {
				var yval = m.intbinop(i);
				if (MATCH_NEG && m.xconst && m.xint == 0) return emit1(I_NEGQ | (AM_OP << AM_SHIFT), ovwReg(i, m.y));
				emitSimpleBinop(selectWidth(i, I_SUBD), i, yval);
			}
			IntMul => {
				// XXX: multiply by 1, 2, 3, 4, 5, 9, powers of 2
				var opcode = selectWidth(i, I_MULD);
				m.intbinop(i);
				ovwReg(i, m.x);
				if (tryUseImm32(m.y)) return emitN(opcode | (AM_OP_IMM << AM_SHIFT));
				use(m.y);
				return emitN(opcode | (AM_REG_OP << AM_SHIFT));
			}
			IntDiv => emitIntDivMod(i, false);
			IntMod => emitIntDivMod(i, true);
			BoolAnd => emitSimpleBinop(I_ANDD, i, m.boolbinop(i));
			BoolOr => emitSimpleBinop(I_ORD, i, m.boolbinop(i));
			IntAnd => emitSimpleBinop(selectWidth(i, I_ANDD), i, m.intbinop(i));
			IntOr => emitSimpleBinop(selectWidth(i, I_ORD), i, m.intbinop(i));
			IntXor => emitSimpleBinop(selectWidth(i, I_XORD), i, m.intbinop(i));
			IntShl => emitShift(i, selectWidth(i, I_SHLD));
			IntSar => emitShift(i, selectWidth(i, I_SARD));
			IntShr => emitShift(i, selectWidth(i, I_SHRD));
			IntViewI => emitIntViewI(i);
			FloatViewI(is64) => {
				emit2(if(is64, I_MOVSD, I_MOVSS) | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			IntCastF(is64) => doIntConvertF(i, is64);
			FloatPromoteI(is64) => doFloatRoundI(i, is64);
			FloatRoundI(is64) => doFloatRoundI(i, is64);
			FloatRound(is64) => {
				var round = int.view(X86_64Rounding.TO_NEAREST.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatAbs(is64) => {
				var t = newTmp(if(is64, Float.FLOAT64, Float.FLOAT32));
				emit2(if(is64, I_PSLLQ, I_PSLLD) | (AM_XMM_IMM << AM_SHIFT), ovwv(t, getVReg(i.input0()), Regs.XMM_CLASS), useInt(1));
				emit2(if(is64, I_PSRLQ, I_PSRLD) | (AM_XMM_IMM << AM_SHIFT), ovwv(getVReg(i), t, Regs.XMM_CLASS), useInt(1));
			}
			FloatCeil(is64) => {
				var round = int.view(X86_64Rounding.TO_POS_INF.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatFloor(is64) => {
				var round = int.view(X86_64Rounding.TO_NEG_INF.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatRoundD => {
				emit2(I_CVTSD2SS | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			IntViewF(is64) => {
				emit2(if(is64, I_MOVSD, I_MOVSS) | (AM_REG_XMM << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			IntTruncF(is64) => doIntConvertF(i, is64);
			FloatPromoteF => {
				emit2(I_CVTSS2SD | (AM_XMM_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
			}
			BoolEq,
			BoolNot,
			RefEq,
			IntEq,
			IntLt,
			IntLteq,
			PtrLt,
			PtrLteq => {
				var cmp = matchCmp(i, false);
				emitCmp(cmp);
				emit1(ArchInstrs.FLAG_NO_GAP | I_SETC | encodeCond(cmp.cond), dfnFixed(i, Regs.GPR_CLASS));
			}
			FloatAdd(is64) => doFloatBinop(i, if(is64, I_ADDSD, I_ADDSS));
			FloatSub(is64) => doFloatBinop(i, if(is64, I_SUBSD, I_SUBSS));
			FloatMul(is64) => doFloatBinop(i, if(is64, I_MULSD, I_MULSS));
			FloatDiv(is64) => doFloatBinop(i, if(is64, I_DIVSD, I_DIVSS));
			FloatBitEq(is64) => {
				var t1 = newTmp(i.op.typeArgs[0]), t2 = newTmp(Int.TYPE);
				emit2(if(is64, I_PCMPEQQ, I_PCMPEQD) | (AM_XMM_OP << AM_SHIFT), ovwv(t1, getVReg(i.input0()), Regs.XMM_CLASS), use(i.input1()));
				emit2(I_MOVSD | (AM_REG_XMM << AM_SHIFT), dfnv(t2, Regs.GPR_CLASS), usev(t1, Regs.XMM_CLASS));
				emit2(I_ANDD | (AM_OP_IMM << AM_SHIFT), ovwv(getVReg(i), t2, Regs.GPR_CLASS), useInt(1));
			}
			FloatEq(is64) => doFloatCmp(i, is64, X86_64Conds.Z);
			FloatNe(is64) => doFloatCmp(i, is64, X86_64Conds.NZ);
			FloatLt(is64) => doFloatCmp(i, is64, X86_64Conds.C);
			FloatLteq(is64) => doFloatCmp(i, is64, X86_64Conds.NA);
			FloatSqrt(is64) => {
				emit2(if(is64, I_SQRTSD, I_SQRTSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
			}
			ConditionalThrow(exception) => {
				var cmp = matchCmp(i.input0(), true);
				emitCmp(cmp);
				emit1(ArchInstrs.FLAG_NO_GAP | I_THROWC | encodeCond(cmp.cond), useExSource(exception, i.source));
			}
			PtrLoad => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					return emitN(I_TESTQ | (AM_MRRSD_IMM << AM_SHIFT));
				}
				var opcode: int;
				match (size) {
					1 => opcode = if(V3.isSigned(lt), I_MOVBSX, I_MOVBZX) | (AM_REG_MRRSD << AM_SHIFT);
					2 => opcode = if(V3.isSigned(lt), I_MOVWSX, I_MOVWZX) | (AM_REG_MRRSD << AM_SHIFT);
					4 => opcode = if(lt.typeCon.kind == V3Kind.FLOAT,
						I_MOVSS | (AM_XMM_MRRSD << AM_SHIFT),
						I_MOVD | (AM_REG_MRRSD << AM_SHIFT));
					8 => opcode = if(lt.typeCon.kind == V3Kind.FLOAT,
						I_MOVSD | (AM_XMM_MRRSD << AM_SHIFT),
						I_MOVQ | (AM_REG_MRRSD << AM_SHIFT));
					_ => context.fail("invalid size for load");
				}
				dfnReg(i);
				useMrrsd(t);
				// TODO: source position for NullCheckException
				return emitN(opcode);
			}
			PtrStore => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				useMrrsd(t);
				var y = i.input1();
				if (tryUseImm32(y)) {
					// store of constant to memory; ignore floating point if imm32
					var opcode: int;
					match (size) {
						0 => opcode = I_TESTQ;
						1 => opcode = I_MOVB;
						2 => opcode = I_MOVW;
						4 => opcode = I_MOVD;
						8 => opcode = I_MOVQ;
						_ => context.fail("invalid size for store");
					}
					emitN(opcode | (AM_MRRSD_IMM << AM_SHIFT)); // TODO: source pos
				} else {
					var opcode: int;
					match (size) {
						0 => opcode = I_TESTQ | (AM_MRRSD_REG << AM_SHIFT);
						1 => opcode = I_MOVB | (AM_MRRSD_REG << AM_SHIFT);
						2 => opcode = I_MOVW | (AM_MRRSD_REG << AM_SHIFT);
						4 => opcode = if(lt.typeCon.kind == V3Kind.FLOAT,
							I_MOVSS | (AM_MRRSD_XMM << AM_SHIFT),
							I_MOVD | (AM_MRRSD_REG << AM_SHIFT));
						8 => opcode = if(lt.typeCon.kind == V3Kind.FLOAT,
							I_MOVSD | (AM_MRRSD_XMM << AM_SHIFT),
							I_MOVQ | (AM_MRRSD_REG << AM_SHIFT));
						_ => context.fail("invalid size for store");
					}
					useReg(y);
					emitN(opcode); // TODO: source pos
				}
			}
			PtrCmpSwp => {
				var lt = i.op.typeArgs[0], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				var opcode: int;
				useMrrsd(t);
				var expect = useFixed(i.input1(), Regs.RAX);
				useFixed(i.input1(), Regs.NOT_RAX);
				match (size) {
					1 => opcode = I_CMPXCHG8;
					2 => opcode = I_CMPXCHG16;
					4 => opcode = I_CMPXCHG32;
					8 => opcode = I_CMPXCHG64;
					_ => context.fail("invalid size for cmpswp");
				}
				// TODO: source position for NullCheckException
				emitN(opcode | (AM_MRRSD_REG << AM_SHIFT)); // TODO: addressing mode is wrong
				emit1(ArchInstrs.FLAG_NO_GAP | I_SETC | encodeCond(X86_64Conds.Z), dfnReg(i));
			}
			PtrAdd => {
				// XXX: use binop matching for PtrAdd
				emit2(I_ADDQ | (AM_REG_OP << AM_SHIFT), ovwReg(i, i.input0()), use(i.input1()));
			}
			PtrSub => {
				// XXX: use binop matching for PtrAdd
				emit2(I_SUBQ | (AM_REG_OP << AM_SHIFT), ovwReg(i, i.input0()), use(i.input1()));
			}
			Alloc => {
				// TODO: hardwired to test allocation routine
				emit1(I_TEST_ALLOC | (AM_OP << AM_SHIFT), ovwReg(i, i.input0()));
			}
			CallAddress(funcRep) => emitCall(i, funcRep);
			CallerIp => {
				emit1(I_CALLER_IP, dfnReg(i));
			}
			CallerSp => {
				emit1(I_CALLER_SP, dfnReg(i));
			}
			CallKernel => ; // TODO
			TupleGetElem => ; // do nothing; calls will define their projections
			_ => return context.fail1("unexpected opcode %s", i.op.opcode.name);
		}
	}
	def doIntConvertF(i: SsaApplyOp, is64: bool) {
		var it = IntType.!(i.op.typeArgs[1]);
		var t1 = newTmp(if(is64, Float.FLOAT64, Float.FLOAT32));
		var round = int.view(RoundingMode.TO_ZERO.tag) << ROUNDING_SHIFT;
		emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnv(t1, Regs.XMM_CLASS), use(i.input0()));
		match (it.rank) {
			SUBU32, SUBI32, I32 => {
				emit2(if(is64, I_CVTSD2SID, I_CVTSS2SID) | (AM_REG_XMM << AM_SHIFT), dfnReg(i), usev(t1, 0));
			}
			U32, SUBI64, SUBU64, I64 => {
				emit2(if(is64, I_CVTSD2SIQ, I_CVTSS2SIQ) | (AM_REG_XMM << AM_SHIFT), dfnReg(i), usev(t1, 0));
			}
			U64 => {
				context.fail("unimplemented IntCastF -> u64"); // TODO: conversion from float to u64
			}
		}
	}
	def doFloatRoundI(i: SsaApplyOp, is64: bool) {
		var it = IntType.!(i.op.typeArgs[0]);
		match (it.rank) {
			SUBI32, SUBU32, I32 => {
				var opcode = if(is64, I_CVTSI2SDD, I_CVTSI2SSD);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			U32, SUBI64, SUBU64, I64 => {
				var opcode = if(is64, I_CVTSI2SDQ, I_CVTSI2SSQ);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			U64 => {
				context.fail("unimplemented FloatPromoteI<u64>"); //TODO
			}
		}
	}
	def emitIntViewI(i: SsaApplyOp) {
		var ft = IntType.!(i.op.typeArgs[0]);
		var tt = IntType.!(i.op.typeArgs[1]);
		var y = i.input0();
		match (tt.rank) {
			U32,
			I32 => return emitMovd(i); // nop
			SUBU32 => return emit2(I_ANDD | (AM_OP_IMM << AM_SHIFT), ovwReg(i, y), useImm(tt.max));
			SUBI32 => return emitShiftShift(i, I_SHLD, I_SARD, tt.ishift);
			_ => ;
		}
		match (ft.rank) {
			SUBI32, I32 => match (tt.rank) {
				U64,
				I64,
				SUBI64 => emitShiftShift(i, I_SHLQ, I_SARQ, ft.lshift);
				SUBU64 => emitShiftShiftShiftShift(i, I_SHLQ, I_SARQ, 32, I_SHLQ, I_SHRQ, tt.lshift);
				_ => ; // handled above
			}
			SUBU32, U32 => {
				emitMovd(i);
			}
			SUBI64, I64 => match (tt.rank) {
				U64,
				I64 => emitMovd(i);
				SUBU64 => emitShiftShift(i, I_SHLQ, I_SHRQ, tt.lshift);
				SUBI64 => emitShiftShift(i, I_SHLQ, I_SARQ, tt.lshift);
				_ => ; // handled above
			}
			SUBU64, U64 => match (tt.rank) {
				U64,
				I64 => emitMovd(i);
				SUBU64  => emitShiftShift(i, I_SHLQ, I_SHRQ, tt.lshift);
				SUBI64  => emitShiftShift(i, I_SHLQ, I_SARQ, tt.lshift);
				_ => ; // handled above
			}
		}
	}
	def emitShiftShift(i: SsaApplyOp, op1: int, op2: int, width: int) {
		var t = newTmp(i.op.typeArgs[1]), reg = anyReg(t);
		Terminal.put3("emitShiftShift(tmp=v%d, regClass=%s, constraint=%s)\n", t.varNum, t.regClass.name, frame.conv.regSet.identify(reg));
		emit2(op1 | (AM_OP_IMM << AM_SHIFT), ovwv(t, getVReg(i.input0()), reg), useInt(width));
		emit2(op2 | (AM_OP_IMM << AM_SHIFT), ovwv(getVReg(i), t, reg), useInt(width));
	}
	def emitShiftShiftShiftShift(i: SsaApplyOp, op1: int, op2: int, width1: int, op3: int, op4: int, width2: int) {
		var t = i.op.typeArgs[1];
		var t1 = newTmp(t), t2 = newTmp(t), t3 = newTmp(t), reg = anyReg(t1);
		emit2(op1 | (AM_OP_IMM << AM_SHIFT), ovwv(t1, getVReg(i.input0()), reg), useInt(width1));
		emit2(op2 | (AM_OP_IMM << AM_SHIFT), ovwv(t2, t1, reg), useInt(width1));
		emit2(op3 | (AM_OP_IMM << AM_SHIFT), ovwv(t3, t2, reg), useInt(width2));
		emit2(op4 | (AM_OP_IMM << AM_SHIFT), ovwv(getVReg(i), t3, reg), useInt(width2));
	}
	def emitMovd(i: SsaApplyOp) {
		emit2(I_MOVD | (AM_REG_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
	}
	def emitIntDivMod(i: SsaApplyOp, isMod: bool) {
		var it = IntType.!(i.op.typeArgs[0]);
		var x = i.input0();
		var upper: VReg;
		if (it.signed) {
			if (SsaConst.?(x)) { // constant-fold sign-extension of dividend
				var val = Long.unboxSU(SsaConst.!(x).val, true);
				if (val < 0) upper = getVReg(if(it.width > 32, context.graph.longConst(-1), context.graph.intConst(-1)));
				else upper = getVReg(context.graph.zeroConst());
			} else {
				upper = newTmp(it);
				var opcode = if(it.width > 32, I_CQO, I_CDQ);
				emit2(opcode | (AM_OP_REG << AM_SHIFT), dfnv(upper, Regs.RDX), useFixed(i.input0(), Regs.RAX));
			}
		} else {
			upper = getVReg(context.graph.zeroConst());
		}
		dfnFixed(i, if(isMod, Regs.RDX, Regs.RAX));
		kill(-1, if(isMod, Regs.RAX, Regs.RDX));
		useFixed(x, Regs.RAX);
		usev(upper, Regs.RDX);
		useFixed(i.input1(), Regs.NOT_RAX_RDX);
		// TODO: source position for DivideByZeroException
		return emitN(selectWidth(i, if(it.signed, I_IDIVD, I_DIVD)));
	}
	def selectWidth(i: SsaApplyOp, op: int) -> int {
		return if(intOpWidth(i) > 32, op + I_QD_DIFF, op);
	}
	def intOpWidth(i: SsaApplyOp) -> byte {
		// XXX: factor this out and clean it up
		var t = i.op.typeArgs[0];
		if (IntType.?(t)) return IntType.!(t).width;
		if (t.typeCon.kind == V3Kind.ENUM) return V3.getVariantTagType(t).width;
		if (t.typeCon.kind == V3Kind.ENUM_SET) return V3.getEnumSetType(t).width;
		return 64;
	}
	def intOpSize(i: SsaApplyOp) -> byte {
		return byte.view((7 + intOpWidth(i)) >> 3);
	}
	def emitCall(call: SsaApplyOp, funcRep: Mach_FuncRep) {
		var func = call.input0(), mi: MachInstr;
		var conv = frame.allocCallerSpace(X86_64VirgilCallConv.getForFunc(funcRep));

		// define the return value(s) of the call
		var rv = getProjections(call);
		for (i < rv.length) {
			var r = rv[i];
			if (r != null) dfnFixed(r, conv.calleeRet(i));
		}
		// TODO: var lp = if(rtgc != null, livePoint());
		kill(newLivepoint(), Regs.ALL);	// XXX: manually split liveness across call?
		var skip = 0;
		if (SsaConst.?(func)) {
			var target = Address<IrMethod>.!(SsaConst.!(func).val);
			useImm(target);
			if (target == null || target.val == null || V3.isComponent(target.val.receiver)) skip = 1;
		} else {
			useReg(func);
		}

		// use the arguments to the call
		var inputs = call.inputs;
		for (i = 1 + skip; i < inputs.length; i++) {  // input[0] == func
			useFixed(inputs[i].dest, conv.calleeParam(i - 1));
		}
		emitN(I_CALL);
	}
	def emitShift(i: SsaApplyOp, opcode: int) {
		// XXX: match x << (y & mask)
		var yval = m.intbinop(i);
		if (MATCH_OP_I && m.yconst) {
			var shiftor = if(m.wide, u6.view(m.ylong), u5.view(m.yint));
			return emit2(opcode | (AM_OP_IMM << AM_SHIFT), ovwReg(i, m.x), useInt(shiftor));
		}
		return emit2(opcode | (AM_OP << AM_SHIFT), ovwReg(i, m.x), useFixed(m.y, Regs.RCX));
	}
	def useMrrsd(x: SsaInstr, y: SsaInstr, scale: byte, disp: Val) {
		if (x == null) useInt(0); // no base register, will ignore
		else useReg(x);
		if (y == null) useInt(0); // no index register, will ignore
		else useReg(y);
		useInt(scale);
		useImm(disp);
	}
	def tryUseImm32(i: SsaInstr) -> bool {
		if (i == null) { useInt(0); return true; }
		if (SsaConst.?(i)) {
			var val = SsaConst.!(i).val;
			match (val) {
				null => { useImm(val); return true; }
				x: Box<int> => { useImm(val); return true; }
				x: Box<long> => if(x.val == int.view(x.val)) { useInt(int.view(x.val)); return true; }
				x: Addr => { useImm(val); return true; }
				x: Box<bool> => { useInt(if(x.val, 1, 0)); return true; }
			}
		}
		return false;
	}
	def matchMrrsd(i: SsaInstr) -> (SsaInstr, SsaInstr, byte, Val) {
		if (SsaConst.?(i)) return (null, null, 1, SsaConst.!(i).val);
		var t = matchAddImm(i, null), i = t.0, disp = t.1;
		var xadd: SsaApplyOp;
		if (MATCH_ADDR_ADD && (xadd = cover(Opcode.PtrAdd.tag, i)) != null) {
			var x = xadd.input0(), y = xadd.input1();
			var xs = matchScale(x);
			if (xs.1 != 1) return (y, xs.0, xs.1, disp);
			var ys = matchScale(y);
			return (x, ys.0, ys.1, disp);
		}
		var is = matchScale(i);
		return (null, is.0, is.1, disp);
	}
	def matchScale(i: SsaInstr) -> (SsaInstr, byte) {
		if (!MATCH_SCALE) return (i, 1);
		if (!SsaApplyOp.?(i)) return (i, 1);
		var apply = SsaApplyOp.!(i);
		if (Opcode.IntMul.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 1) return (m.x, 1);
			if (yval == 2) return (m.x, 2);
			if (yval == 4) return (m.x, 4);
			if (yval == 8) return (m.x, 8);
		} else if (Opcode.IntShl.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 0) return (m.x, 1);
			if (yval == 1) return (m.x, 2);
			if (yval == 2) return (m.x, 4);
			if (yval == 3) return (m.x, 8);
		}
		return (apply, 1);
	}
	def emitSimpleBinop(opcode: int, i: SsaApplyOp, yval: int) {
		// XXX: select better left operand using liveness
		ovwReg(i, m.x);
		if (tryUseImm32(m.y)) {
			opcode |= (AM_OP_IMM << AM_SHIFT);
		} else { // XXX: cover loads with mrrsd operand
			opcode |= (AM_REG_OP << AM_SHIFT);
			use(m.y);
		}
		emitN(opcode);
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {
		emit1(I_THROW, useExSource(i.exception, i.source));
	}
	def visitIf(block: SsaBlock, i: SsaIf) {
		var key = i.input0(), cmp = matchCmp(key, true), succ = i.block().succ;
		var s0 = succ(0).dest, s1 = succ(1).dest, target: SsaBlock, jmp: SsaBlock;
		if (blocks.isImmediatelyAfter(context.block, s1)) { // fall through to s1
			target = s0;
		} else if (blocks.isImmediatelyAfter(context.block, s0)) {  // fall through to s0
			cmp = cmp.negate();
			target = s1;
		} else {  // cannot fall through
			target = s0;
			jmp = s1;
		}
		emitCmp(cmp);
		emit1(ArchInstrs.FLAG_NO_GAP | I_JC | encodeCond(cmp.cond), useLabel(target));
		if (jmp != null) emit1(ArchInstrs.FLAG_NO_GAP | I_JMP, useLabel(jmp));
	}
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {
		var size = mach.sizeOf(i.keyType);
		use(i.input0());
		useInt(i.minValue);
		useScratch();
		for (s in block.succs()) useLabel(s.dest);
		emitN(if(size > 4, I_SWITCHQ, I_SWITCHD));
	}
	def visitGoto(block: SsaBlock, i: SsaGoto) {
		var target = i.target();
		if (!blocks.isImmediatelyAfter(context.block, target)) {
			emit1(I_JMP, useLabel(target));  // jump to block if not successor
		}
	}

	def genSaveLocal(loc: int, v: VReg) {
		if (Regs.SET.isCallerStack(loc)) return; // defined by caller, nothing to do
		if (Regs.SET.isStack(loc)) {
			var scratch = Regs.SCRATCH_GPR;
			var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ); // no need for XMM for stack-to-stack
			emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, scratch)), op(Operand.Use(null, loc)));
			emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(v, v.spill)), op(Operand.Use(null, scratch)));
			return;
		}
		var opcode: int;
		match (v.regClass) {
			I32 => opcode = I_MOVD | (AM_OP_REG << AM_SHIFT);
			REF, I64 => opcode = I_MOVQ | (AM_OP_REG << AM_SHIFT);
			F32 => opcode = I_MOVSS | (AM_OP_XMM << AM_SHIFT);
			F64 => opcode = I_MOVSD | (AM_OP_XMM << AM_SHIFT);
		}
		emit2(opcode, op(Operand.Def(v, v.spill)), op(Operand.Use(null, loc)));
	}
	def genLoadLocalIntoReg(v: VReg, reg: int) {
		var opcode = if(v.regClass == RegClass.I32 || v.regClass == RegClass.F32, I_MOVD, I_MOVQ);
		if (v.isConst()) {
			var val = SsaConst.!(v.ssa).val;
			match (val) {
				x: Box<long> => if(int.view(x.val) != x.val) return genLoadLongConstIntoReg(x.val, reg);
				x: Float64Val => if(int.view(x.bits) != x.bits) return genLoadLongConstIntoReg(long.view(x.bits), reg);
			}
			emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(null, reg)), op(Operand.Immediate(val)));
			return;
		}
		emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, reg)), op(Operand.Use(v, v.spill)));
	}
	def genLoadLongConstIntoReg(val: long, reg: int) {
		var addr = mach.getLongConstAddr(val);
		op(Operand.Def(null, reg));
		useMrrsd(null, null, 1, addr);
		emitN(I_MOVQ | (AM_REG_MRRSD << AM_SHIFT));
	}
	def genRestoreLocal(v: VReg, loc: int) {
		var from: int;
		if (Regs.SET.isStack(loc)) {
			// stack to stack move
			// XXX: load 32-bit constant directly into stack location with AM_OP_IMM
			var scratch = Regs.SCRATCH_GPR; // use GPR as scratch to load constant
			genLoadLocalIntoReg(v, scratch);
			var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ); // TODO: XMM
			emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, scratch)));
			return;
		}
		var xmm = Regs.toXmmr(loc);
		if (xmm != null) {
			var opcode = if(v.regClass == RegClass.F32, I_MOVSS, I_MOVSD);
			if (v.isConst()) {
				var scratch = Regs.SCRATCH_GPR; // use GPR as scratch to load constant into XMM
				genLoadLocalIntoReg(v, scratch);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, scratch)));
			} else {
				emit2(opcode | (AM_XMM_OP << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, v.spill)));
			}
		} else {
			genLoadLocalIntoReg(v, loc);
		}
	}
	def doFloatBinop(i: SsaApplyOp, opcode: int) {
		var d = getVReg(i), u = getVReg(i.input0()), reg = anyReg(d);
		emit2(opcode | (AM_XMM_OP << AM_SHIFT), ovwv(d, u, reg), use(i.input1()));
	}
	def doFloatCmp(i: SsaApplyOp, is64: bool, cond: X86_64Cond) {
		var opcode = if(is64, I_UCOMISD, I_UCOMISS);
		emit2(opcode | (AM_XMM_OP << AM_SHIFT), useReg(i.input0()), use(i.input1()));
		var t1 = newTmp(Bool.TYPE), t2 = newTmp(Bool.TYPE), gloc = Regs.GPR_CLASS;
		// Use two setc instructions with and/or to implement multiple conditions.
		// To future-proof the generated code against future regalloc opts, allow no gaps
		var nogap = ArchInstrs.FLAG_NO_GAP;
		emit1(nogap | I_SETC | encodeCond(cond), dfnv(t1, gloc));
		if (cond == X86_64Conds.NZ) {
			emit1(nogap | I_SETC | encodeCond(X86_64Conds.P), dfnv(t2, gloc));
			emit2(I_ORD | (AM_REG_OP << AM_SHIFT), ovwv(getVReg(i), t1, gloc), usev(t2, 0));
		} else {
			emit1(nogap | I_SETC | encodeCond(X86_64Conds.NP), dfnv(t2, gloc));
			emit2(I_ANDD | (AM_REG_OP << AM_SHIFT), ovwv(getVReg(i), t1, gloc), usev(t2, 0));
		}
	}
	def emitCmp(cmp: X86_64CmpMatch) {
		var opcode = I_CMPB;
		if (cmp.size > 4) opcode = I_CMPQ;
		else if (cmp.size > 1) opcode = I_CMPD;
		useReg(cmp.x); // XXX: don't use reg if can use immediate for y
		if (tryUseImm32(cmp.y)) {
			emitN(opcode | (AM_OP_IMM << AM_SHIFT));
		} else {
			use(cmp.y);
			emitN(opcode | (AM_REG_OP << AM_SHIFT));
		}
	}


	def assemble(opcode: int, a: Array<Operand>) {
		if (opcode < 0) {
			match (opcode) {
				ArchInstrs.ARCH_ENTRY => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.subq_r_i(X86_64Regs.RSP, adjust); // allocate frame
				}
				ArchInstrs.ARCH_BLOCK => {
					var label = toLabel(a[0]);
					label.pos = asm.pos();
				}
				ArchInstrs.ARCH_RET => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.addq_r_i(X86_64Regs.RSP, adjust); // deallocate frame
					asm.ret();
				}
			}
			return;
		}
		var start = asm.pos(), addr: Addr;
		var mode = ((opcode >> AM_SHIFT) & 0x1F);
		match (mode) {
			AM_NONE => {
				assemble_none(opcode, a);
			}
			AM_OP => {
				var loc = toLoc(a[0]), a = Regs.toGpr(loc);
				if (a != null) assemble_r(opcode, a);
				else assemble_m(opcode, loc_m(loc));
			}
			AM_OP_REG => {
				var loc = toLoc(a[0]), b = toGpr(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_r(opcode, a, b);
				else assemble_m_r(opcode, loc_m(loc), b);
			}
			AM_OP_XMM => {
				var loc = toLoc(a[0]), b = toXmmr(a[1]), a = Regs.toXmmr(loc);
				if (a != null) assemble_s_s(opcode, a, b);
				else assemble_m_s(opcode, loc_m(loc), b);
			}
			AM_OP_IMM => {
				var loc = toLoc(a[0]), imm = toImm(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_i(opcode, a, imm);
				else assemble_m_i(opcode, loc_m(loc), imm);
			}
			AM_REG_OP => {
				var reg = toGpr(a[0]), loc = toLoc(a[1]), b = Regs.toGpr(loc);
				if (b != null) assemble_r_r(opcode, reg, b);
				else assemble_r_m(opcode, reg, loc_m(loc));
			}
			AM_XMM_OP => {
				var reg = toXmmr(a[0]), loc = toLoc(a[1]), b = Regs.toXmmr(loc);
				if (b != null) assemble_s_s(opcode, reg, b);
				else assemble_s_m(opcode, reg, loc_m(loc));
			}
			AM_XMM_IMM => {
				var reg = toXmmr(a[0]), imm = toB32(toImm(a[1]));
				match (opcode & 0xFF) {
					I_PSLLD => asm.pslld_i(reg, u5.view(imm));
					I_PSRLD => asm.psrld_i(reg, u5.view(imm));
					I_PSLLQ => asm.psllq_i(reg, u6.view(imm));
					I_PSRLQ => asm.psrlq_i(reg, u6.view(imm));
				}
			}
			AM_XMM_REG => {
				assemble_s_r(opcode, toXmmr(a[0]), toGpr(a[1]));
			}
			AM_REG_XMM => {
				assemble_r_s(opcode, toGpr(a[0]), toXmmr(a[1]));
			}
			AM_MRRSD_REG => {
				var m = toMrrsd(a, 0);
				assemble_m_r(opcode, m, toGpr(a[4]));
			}
			AM_MRRSD_XMM => {
				var m = toMrrsd(a, 0);
				assemble_m_s(opcode, m, toXmmr(a[4]));
			}
			AM_MRRSD_IMM => {
				var m = toMrrsd(a, 0);
				assemble_m_i(opcode, m, toImm(a[4]));
			}
			AM_REG_MRRSD => {
				var m = toMrrsd(a, 1);
				assemble_r_m(opcode, toGpr(a[0]), m);
			}
			AM_XMM_MRRSD => {
				var m = toMrrsd(a, 1);
				assemble_s_m(opcode, toXmmr(a[0]), m);
			}
			_ => return context.fail1("unknown addressing mode %d", mode);
		}
	}
	def assemble_r(opcode: int, a: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.negd_r(a);
			I_NOTD => asm.notd_r(a);
			I_MULD => asm.imuld_r(a);
			I_INCD => asm.incd_r(a);
			I_DECD => asm.decd_r(a);
			I_SHLD => asm.shld_r_cl(a);
			I_SARD => asm.sard_r_cl(a);
			I_SHRD => asm.shrd_r_cl(a);
			I_DIVD => asm.divd_r(a);
			I_IDIVD => asm.idivd_r(a);

			I_NEGQ => asm.negq_r(a);
			I_NOTQ => asm.notq_r(a);
			I_MULQ => asm.imulq_r(a);
			I_INCQ => asm.incq_r(a);
			I_DECQ => asm.decq_r(a);
			I_SHLQ => asm.shlq_r_cl(a);
			I_SARQ => asm.sarq_r_cl(a);
			I_SHRQ => asm.shrq_r_cl(a);
			I_DIVQ => asm.divq_r(a);
			I_IDIVQ => asm.idivq_r(a);
			I_TEST_ALLOC => {
				var addr = X86_64AddrRef.new(null, null, 1, CiRuntimeModule.HEAP_CUR_LOC);
				asm.xaddq_m_r(addr, a);
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m(opcode: int, a: X86_64Addr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.negd_m(a);
			I_NOTD => asm.notd_m(a);
			I_MULD => asm.imuld_m(a);
			I_INCD => asm.incd_m(a);
			I_DECD => asm.decd_m(a);
			I_SHLD => asm.shld_m_cl(a);
			I_SARD => asm.sard_m_cl(a);
			I_SHRD => asm.shrd_m_cl(a);
			I_DIVD => asm.divq_m(a); // TODO
			I_IDIVD => asm.idivq_m(a); // TODO

			I_NEGQ => asm.negq_m(a);
			I_NOTQ => asm.notq_m(a);
			I_MULQ => asm.imulq_m(a);
			I_INCQ => asm.incq_m(a);
			I_DECQ => asm.decq_m(a);
			I_SHLQ => asm.shlq_m_cl(a);
			I_SARQ => asm.sarq_m_cl(a);
			I_SHRQ => asm.shrq_m_cl(a);
			I_DIVQ => asm.divq_m(a);
			I_IDIVQ => asm.idivq_m(a);

			I_FLDD => asm.fldd(a);
			I_FLDQ => asm.fldq(a);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_r_r(opcode: int, a: X86_64Gpr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_r(a, b);
			I_ORD => asm.ord_r_r(a, b);
			I_ADCD => asm.adcd_r_r(a, b);
			I_ANDD => asm.andd_r_r(a, b);
			I_SUBD => asm.subd_r_r(a, b);
			I_XORD => asm.xord_r_r(a, b);
			I_CMPD => asm.cmpd_r_r(a, b);
			I_CMPB => asm.cmpb_r_r(a, b);
			I_MULD => asm.imuld_r_r(a, b);
			I_CDQ => asm.cdq();

			I_ADDQ => asm.addq_r_r(a, b);
			I_ORQ => asm.orq_r_r(a, b);
			I_ADCQ => asm.adcq_r_r(a, b);
			I_ANDQ => asm.andq_r_r(a, b);
			I_SUBQ => asm.subq_r_r(a, b);
			I_XORQ => asm.xorq_r_r(a, b);
			I_CMPQ => asm.cmpq_r_r(a, b);
			I_MULQ => asm.imulq_r_r(a, b);
			I_CQO => asm.cqo();

			I_MOVD => asm.movd_r_r(a, b);
			I_MOVQ => asm.movq_r_r(a, b);
			I_MOVB => asm.movb_r_r(a, b);
			I_MOVBSX => asm.movbsx_r_r(a, b);
			I_MOVBZX => asm.movbzx_r_r(a, b);
			I_MOVWSX => asm.movwsx_r_r(a, b);
			I_MOVWZX => asm.movwzx_r_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_s_s(opcode: int, a: X86_64Xmmr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_ADDSS => asm.addss_s_s(a, b);
			I_SUBSS => asm.subss_s_s(a, b);
			I_MULSS => asm.mulss_s_s(a, b);
			I_DIVSS => asm.divss_s_s(a, b);
			I_SQRTSS => asm.sqrtss_s_s(a, b);
			I_ADDSD => asm.addsd_s_s(a, b);
			I_SUBSD => asm.subsd_s_s(a, b);
			I_MULSD => asm.mulsd_s_s(a, b);
			I_DIVSD => asm.divsd_s_s(a, b);
			I_SQRTSD => asm.sqrtsd_s_s(a, b);
			I_CVTSS2SD => asm.cvtss2sd_s_s(a, b);
			I_CVTSD2SS => asm.cvtsd2ss_s_s(a, b);
			I_PCMPEQD => asm.pcmpeqd_s_s(a, b);
			I_PCMPEQQ => asm.pcmpeqq_s_s(a, b);
			I_UCOMISS => asm.ucomiss_s_s(a, b);
			I_UCOMISD => asm.ucomisd_s_s(a, b);
			I_ROUNDSS => asm.roundss_s_s(a, b, decodeRounding(opcode));
			I_ROUNDSD => asm.roundsd_s_s(a, b, decodeRounding(opcode));
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_s_m(opcode: int, a: X86_64Xmmr, b: X86_64Addr) {
		match (opcode & 0xFF) {
			I_ADDSS => asm.addss_s_m(a, b);
			I_SUBSS => asm.subss_s_m(a, b);
			I_MULSS => asm.mulss_s_m(a, b);
			I_DIVSS => asm.divss_s_m(a, b);
			I_SQRTSS => asm.sqrtss_s_m(a, b);
			I_ADDSD => asm.addsd_s_m(a, b);
			I_SUBSD => asm.subsd_s_m(a, b);
			I_MULSD => asm.mulsd_s_m(a, b);
			I_DIVSD => asm.divsd_s_m(a, b);
			I_SQRTSD => asm.sqrtsd_s_m(a, b);
			I_MOVSS => asm.movss_s_m(a, b);
			I_MOVSD => asm.movsd_s_m(a, b);
			I_CVTSS2SD => asm.cvtss2sd_s_m(a, b);
			I_CVTSD2SS => asm.cvtsd2ss_s_m(a, b);
			I_PCMPEQD => asm.pcmpeqd_s_m(a, b);
			I_PCMPEQQ => asm.pcmpeqq_s_m(a, b);
			I_CVTSI2SSD => asm.cvtsi2ssd_s_m(a, b);
			I_CVTSI2SSQ => asm.cvtsi2ssq_s_m(a, b);
			I_CVTSI2SDD => asm.cvtsi2sdd_s_m(a, b);
			I_CVTSI2SDQ => asm.cvtsi2sdq_s_m(a, b);
			I_UCOMISS => asm.ucomiss_s_m(a, b);
			I_UCOMISD => asm.ucomisd_s_m(a, b);
			I_ROUNDSS => asm.roundss_s_m(a, b, decodeRounding(opcode));
			I_ROUNDSD => asm.roundsd_s_m(a, b, decodeRounding(opcode));
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_r(opcode: int, a: X86_64Addr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_m_r(a, b);
			I_ORD =>  asm.ord_m_r(a, b);
			I_ADCD => asm.adcd_m_r(a, b);
			I_ANDD => asm.andd_m_r(a, b);
			I_SUBD => asm.subd_m_r(a, b);
			I_XORD => asm.xord_m_r(a, b);
			I_CMPD => asm.cmpd_m_r(a, b);
			I_CMPB => asm.cmpb_m_r(a, b);
			I_TESTD => asm.testd_m_r(a, b);

			I_ADDQ => asm.addq_m_r(a, b);
			I_ORQ =>  asm.orq_m_r(a, b);
			I_ADCQ => asm.adcq_m_r(a, b);
			I_ANDQ => asm.andq_m_r(a, b);
			I_SUBQ => asm.subq_m_r(a, b);
			I_XORQ => asm.xorq_m_r(a, b);
			I_CMPQ => asm.cmpq_m_r(a, b);
			I_TESTQ => asm.testq_m_r(a, b);

			I_MOVD => asm.movd_m_r(a, b);
			I_MOVQ => asm.movq_m_r(a, b);
			I_MOVB => asm.movb_m_r(a, b);
			I_MOVW => asm.movw_m_r(a, b);

			I_CMPXCHG8 => asm.cmpxchgb_m_r(a, b);
			I_CMPXCHG16 => asm.cmpxchgw_m_r(a, b);
			I_CMPXCHG32 => asm.cmpxchgd_m_r(a, b);
			I_CMPXCHG64 => asm.cmpxchgq_m_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_s(opcode: int, a: X86_64Addr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movss_m_s(a, b);
			I_MOVSD => asm.movsd_m_s(a, b);
		}
	}
	def assemble_s_r(opcode: int, a: X86_64Xmmr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movd_s_r(a, b);
			I_MOVSD => asm.movq_s_r(a, b);
			I_CVTSI2SSD => asm.cvtsi2ssd_s_r(a, b);
			I_CVTSI2SSQ => asm.cvtsi2ssq_s_r(a, b);
			I_CVTSI2SDD => asm.cvtsi2sdd_s_r(a, b);
			I_CVTSI2SDQ => asm.cvtsi2sdq_s_r(a, b);
		}
	}
	def assemble_r_s(opcode: int, a: X86_64Gpr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movd_r_s(a, b);
			I_MOVSD => asm.movq_r_s(a, b);
			I_CVTSS2SID => asm.cvtss2sid_r_s(a, b);
			I_CVTSS2SIQ => asm.cvtss2siq_r_s(a, b);
			I_CVTSD2SID => asm.cvtsd2sid_r_s(a, b);
			I_CVTSD2SIQ => asm.cvtsd2siq_r_s(a, b);
		}
	}
	def assemble_r_m(opcode: int, a: X86_64Gpr, b: X86_64Addr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_m(a, b);
			I_ORD =>  asm.ord_r_m(a, b);
			I_ADCD => asm.adcd_r_m(a, b);
			I_ANDD => asm.andd_r_m(a, b);
			I_SUBD => asm.subd_r_m(a, b);
			I_XORD => asm.xord_r_m(a, b);
			I_CMPD => asm.cmpd_r_m(a, b);
			I_CMPB => asm.cmpb_r_m(a, b);
			I_TESTD => asm.testq_r_m(a, b);

			I_ADDQ => asm.addq_r_m(a, b);
			I_ORQ =>  asm.orq_r_m(a, b);
			I_ADCQ => asm.adcq_r_m(a, b);
			I_ANDQ => asm.andq_r_m(a, b);
			I_SUBQ => asm.subq_r_m(a, b);
			I_XORQ => asm.xorq_r_m(a, b);
			I_CMPQ => asm.cmpq_r_m(a, b);
			I_TESTQ => asm.testq_r_m(a, b);

			I_MOVD => asm.movd_r_m(a, b);
			I_MOVQ => asm.movq_r_m(a, b);
			I_MOVB => asm.movb_r_m(a, b);
			I_MOVBSX => asm.movbsx_r_m(a, b);
			I_MOVBZX => asm.movbzx_r_m(a, b);
			I_MOVW => asm.movw_r_m(a, b);
			I_MOVWSX => asm.movwsx_r_m(a, b);
			I_MOVWZX => asm.movwzx_r_m(a, b);

			I_CVTSS2SID => asm.cvtss2sid_r_m(a, b);
			I_CVTSS2SIQ => asm.cvtss2siq_r_m(a, b);
			I_CVTSD2SID => asm.cvtsd2sid_r_m(a, b);
			I_CVTSD2SIQ => asm.cvtsd2siq_r_m(a, b);
//TODO			I_CMPXCHG8 => asm.cmpxchgb_r_m(a, b);
//TODO			I_CMPXCHG16 => asm.cmpxchgw_r_m(a, b);
//TODO			I_CMPXCHG32 => asm.cmpxchgd_r_m(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_i(opcode: int, a: X86_64Addr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_m_i(a, b);
			I_ORD =>  asm.ord_m_i(a, b);
			I_ADCD => asm.adcd_m_i(a, b);
			I_ANDD => asm.andd_m_i(a, b);
			I_SUBD => asm.subd_m_i(a, b);
			I_XORD => asm.xord_m_i(a, b);
			I_CMPD => asm.cmpd_m_i(a, b);
			I_CMPB => asm.cmpb_m_i(a, b);
			I_TESTD => asm.testd_m_i(a, b);

			I_ADDQ => asm.addq_m_i(a, b);
			I_ORQ =>  asm.orq_m_i(a, b);
			I_ADCQ => asm.adcq_m_i(a, b);
			I_ANDQ => asm.andq_m_i(a, b);
			I_SUBQ => asm.subq_m_i(a, b);
			I_XORQ => asm.xorq_m_i(a, b);
			I_CMPQ => asm.cmpq_m_i(a, b);
			I_MOVD => asm.movd_m_i(a, b);
			I_MOVQ => asm.movq_m_i(a, b);
			I_TESTQ => asm.testq_m_i(a, b);
			I_MOVB => asm.movb_m_i(a, b);
			I_MOVW => asm.movw_m_i(a, b);
//TODO			I_CMPXCHG8 => asm.cmpxchgb_m_i(a, b);
//TODO			I_CMPXCHG16 => asm.cmpxchgw_m_i(a, b);
//TODO			I_CMPXCHG32 => asm.cmpxchgd_m_i(a, b);
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_r_i(opcode: int, a: X86_64Gpr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_i(a, b);
			I_ORD =>  asm.ord_r_i(a, b);
			I_ADCD => asm.adcd_r_i(a, b);
			I_ANDD => asm.andd_r_i(a, b);
			I_SUBD => asm.subd_r_i(a, b);
			I_XORD => asm.xord_r_i(a, b);
			I_CMPD => asm.cmpd_r_i(a, b);
			I_CMPB => asm.cmpb_r_i(a, b);
			I_MULD => asm.imuld_r_i(a, b);
			I_SHLD => asm.shld_r_i(a, u5.view(b));
			I_SARD => asm.sard_r_i(a, u5.view(b));
			I_SHRD => asm.shrd_r_i(a, u5.view(b));

			I_ADDQ => asm.addq_r_i(a, b);
			I_ORQ =>  asm.orq_r_i(a, b);
			I_ADCQ => asm.adcq_r_i(a, b);
			I_ANDQ => asm.andq_r_i(a, b);
			I_SUBQ => asm.subq_r_i(a, b);
			I_XORQ => asm.xorq_r_i(a, b);
			I_CMPQ => asm.cmpq_r_i(a, b);
			I_MULQ => asm.imulq_r_i(a, b);
			I_MOVD => asm.movd_r_i(a, b);
			I_MOVQ => asm.movq_r_i(a, b);
			I_SHLQ => asm.shlq_r_i(a, u6.view(b));
			I_SARQ => asm.sarq_r_i(a, u6.view(b));
			I_SHRQ => asm.shrq_r_i(a, u6.view(b));
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_none(opcode: int, a: Array<Operand>) {
		match (opcode & 0xFF) {
			I_DIVQ, I_IDIVQ, I_DIVD, I_IDIVD => {
				// TODO: add trap location
				var loc = toLoc(a[4]), reg = Regs.toGpr(loc);
				if (reg != null) assemble_r(opcode, reg);
				else assemble_m(opcode, loc_m(loc));
			}
			I_JMP => {
				var label = toLabel(a[0]);
				if (label.pos >= 0) asm.jmp_rel(label.pos - asm.pos());
				else asm.jmp_rel(REL_MARKER).recordRelLabel32(asm.pos() - 4, label); // XXX: make automatic
			}
			I_JC => {
				var label = toLabel(a[0]), cond = decodeCond(opcode);
				if (label.pos >= 0) asm.jc_rel(cond, label.pos - asm.pos());
				else asm.jc_rel(cond, REL_MARKER).recordRelLabel32(asm.pos() - 4, label); // XXX: make automatic
			}
			I_SETC => {
				var reg = toGpr(a[0]), cond = decodeCond(opcode);
				asm.set_r(cond, reg);
				asm.movbzx_r_r(reg, reg); // XXX: remove zero extend for setc?
			}
			I_THROW => {
				var exSource = toExSource(a[0]);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jmp_rel_addr(X86_64AddrRef.new(null, null, 1, addr));
			}
			I_THROWC => {
				var exSource = toExSource(a[0]), cond = decodeCond(opcode);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jc_rel_addr(cond, X86_64AddrRef.new(null, null, 1, addr));
			}
			I_NEGQ => {
				asm.negq_r(toGpr(a[0]));
			}
			I_SWITCHD, I_SWITCHQ => {
				var is64 = (opcode & 0xFF) == I_SWITCHQ;
				var size = a.length - 4;
				var key = toGpr(a[0]), minValue = Int.unbox(toImm(a[1])), scratchReg = X86_64Regs.RBP; // TODO: scratch reg
				asm.movq_r_r(scratchReg, key);
				if (minValue != 0) {
					if (is64) asm.subq_r_i(scratchReg, minValue);
					else asm.subd_r_i(scratchReg, minValue);
				}
				if (is64) asm.cmpq_r_i(scratchReg, size - 1);
				else asm.cmpq_r_i(scratchReg, size - 1);
				asm.jc_rel(X86_64Conds.A, REL_MARKER).recordRelLabel32(asm.pos() - 4, toLabel(a[a.length - 1])); // XXX: make automatic
				// load from the jump table to follow
				var jtAddr = Addr.new(mach.codeRegion, null, 0);
				asm.movd_r_m(scratchReg, X86_64AddrRef.new(null, scratchReg, 4, jtAddr));
				asm.ijmp_r(scratchReg);
				// align and emit the jump table
				w.align(4);
				jtAddr.absolute = w.endAddr();
				// emit (32-bit) jump table
				for (i = 3; i < a.length; i++) {
					w.zeroN(4);
					asm.recordAbsLabel32(asm.pos() - 4, toLabel(a[i]));
				}
			}
			I_CALL => {
				for (o in a) {
					match (o) {
						Immediate(val) => {
							asm.callr_addr(X86_64AddrRef.new(null, null, 0, Addr.!(val)));
							return;
						}
						Use(vreg, assignment) => {
							asm.icall_r(loc_r(assignment));
							return;
						}
						_ => ;
					}
				}
				context.fail("no target for call");
			}
			I_CALLER_IP => {
				asm.movd_r_m(toGpr(a[0]), X86_64Regs.RSP.plus(frameAdjust()));
			}
			I_CALLER_SP => {
				asm.leaq(toGpr(a[0]), X86_64Regs.RSP.plus(frame.size()));
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def toB32(val: Val) -> int {
		var addr: Addr, b: int;
		match (val) {
			x: Box<int> 	=> b = x.val;
			x: Box<long> 	=> b = int.view(x.val);
			x: Addr 	=> b = ABS_MARKER;
			x: Box<bool> 	=> b = if(x.val, 1);
			x: Float32Val 	=> b = int.view(x.bits);
			x: Float64Val 	=> b = int.view(x.bits);
			null 		=> b = 0;
			_ => ; // TODO
		}
		return b;
	}
	def recordAddr(val: Val) {
		if (Addr.?(val)) {
			// record an absolute patch
			asm.recordAbs32(w.pos - 4, X86_64AddrRef.new(null, null, 0, Addr.!(val)));
		}
	}
	def invalidOpcode(opcode: int) {
		var cond = byte.view(0xF & (opcode >> COND_SHIFT));
		var am = byte.view(0x1F & (opcode >> AM_SHIFT));
		var code = byte.view(opcode);
		context.fail(Strings.format3("invalid opcode cond=%x am=%x opcode=%x", cond, am, code));
	}
	def toMrrsd(a: Array<Operand>, start: int) -> X86_64Addr {
		var base: X86_64Gpr, index: X86_64Gpr, b = a[start + 0], i = a[start + 1];
		if (Operand.Use.?(b)) base = toGpr(b);
		if (Operand.Use.?(i)) index = toGpr(i);
		var scale = byte.view(toInt(a[start + 2]));
		var val = toImm(a[start + 3]);
		if (Addr.?(val)) return X86_64AddrRef.new(base, index, scale, Addr.!(val));
		else return X86_64Addr.new(base, index, scale, Int.unbox(val));
	}

	def matchCmp(i: SsaInstr, inblock: bool) -> X86_64CmpMatch {
		if ((inblock && !inSameBlock(i)) || !SsaApplyOp.?(i)) return X86_64CmpMatch.new(1, Conds.NZ, i, null);
		var apply = SsaApplyOp.!(i);
		match (apply.op.opcode) {
			BoolEq => 	return newCmp(1, i, Conds.Z);
			RefEq =>	return newCmp(mach.refSize, i, Conds.Z);
			IntEq => 	return newCmp(intOpSize(apply), i, Conds.Z);
			IntLt =>	return newCmp(intOpSize(apply), i, signedCmp(i, Conds.L, Conds.C));
			IntLteq =>	return newCmp(intOpSize(apply), i, signedCmp(i, Conds.LE, Conds.NA));
			PtrLt =>	return newCmp(8, i, Conds.C);
			PtrLteq =>	return newCmp(8, i, Conds.NA);
			BoolNot => 	return matchCmp(i.input0(), inblock).negate();
			_ => ;
		}
		return X86_64CmpMatch.new(1, Conds.NZ, i, null);
	}
	def signedCmp(i: SsaInstr, signed: X86_64Cond, unsigned: X86_64Cond) -> X86_64Cond {
		return if(V3.isSigned(SsaApplyOp.!(i).op.sig.paramTypes[0]), signed, unsigned);
	}
	def newCmp(size: byte, i: SsaInstr, cond: X86_64Cond) -> X86_64CmpMatch {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(x) && cond.commute != null) return X86_64CmpMatch.new(size, cond.commute, y, x);
		return X86_64CmpMatch.new(size, cond, x, y);
	}

	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}
	def toGpr(o: Operand) -> X86_64Gpr {
		return loc_r(toLoc(o));
	}
	def toXmmr(o: Operand) -> X86_64Xmmr {
		return loc_s(toLoc(o));
	}
	def toLoc(o: Operand) -> int{
		match (o) {
			Overwrite(dst, src, assignment) => return assignment;
			Def(vreg, assignment) => return assignment;
			Use(vreg, assignment) => return assignment;
			_ => return V3.fail("expected operand with assignment");
		}
	}

	def encodeCond(cond: X86_64Cond) -> int {
		return cond.index << COND_SHIFT;
	}
	def decodeCond(opcode: int) -> X86_64Cond {
		return Conds.all[(opcode >> COND_SHIFT) & 0xF];
	}
	def decodeRounding(opcode: int) -> X86_64Rounding {
		match ((opcode >> ROUNDING_SHIFT) & 0x3) {
			X86_64Rounding.TO_NEAREST.tag => return X86_64Rounding.TO_NEAREST;
			X86_64Rounding.TO_POS_INF.tag => return X86_64Rounding.TO_POS_INF;
			X86_64Rounding.TO_NEG_INF.tag => return X86_64Rounding.TO_NEG_INF;
			X86_64Rounding.TO_ZERO.tag => return X86_64Rounding.TO_ZERO;
		}
		return X86_64Rounding.TO_NEAREST;
	}

	def loc_r(loc: int) -> X86_64Gpr {
		var gpr = Regs.toGpr(loc);
		if (gpr == null) return V3.fail1("expected GPR, got %s", Regs.SET.identify(loc));
		return gpr;
	}
	def loc_s(loc: int) -> X86_64Xmmr {
		var xmm = Regs.toXmmr(loc);
		if (xmm == null) return V3.fail1("expected XMM, got %s", Regs.SET.identify(loc));
		return xmm;
	}
	def loc_m(loc: int) -> X86_64Addr {
		loc = frame.un64(loc);
		var regSet = frame.conv.regSet, wordSize = mach.data.addressSize, offset = 0;
		if (loc >= regSet.calleeStart) {
			offset = wordSize * (loc - regSet.calleeStart);
		} else if (loc >= regSet.callerStart) {
			offset = frame.size() + (wordSize * (loc - regSet.callerStart));
		} else if (loc >= regSet.spillStart) {
			offset = wordSize * (loc - regSet.spillStart + frame.spillArgs);
		} else {
			return V3.fail1("invalid spill location %s", Regs.SET.identify(loc));
		}
		return X86_64Regs.RSP.plus(offset);
	}
	def getOutput() -> ArchInstrBuffer {
		if (out != null) return out;
		return out = X86InstrBuffer.new(this, context.prog, regSet);
	}
}
class X86InstrBuffer extends ArchInstrBuffer {
	def x86codegen: SsaX86_64Gen;
	new(x86codegen, prog: Program, regSet: MachRegSet) super(x86codegen, prog, regSet) { }
	def putArchInstr(indent: int, i: ArchInstr) -> int {
		var opcode = int.view(i.opcode()), a = i.operands;
		var name: string, cond: X86_64Cond, rounding = false;

		match (opcode & 0xFF) {
			I_SWITCHD, I_SWITCHQ => name = "switch";
			I_ADDD => name = "addd";
			I_ORD =>  name = "ord";
			I_ADCD => name = "adcd";
			I_ANDD => name = "andd";
			I_SUBD => name = "subd";
			I_CMPD => name = "cmpd";
			I_CMPB => name = "cmpb";
			I_MULD => name = "imuld";
			I_DIVD => name = "divd";
			I_IDIVD => name = "idivd";
			I_NEGD => name = "negd";
			I_NOTD => name = "notd";
			I_TESTD => name = "testd";
			I_CDQ => name = "cdq";
			I_ADDQ => name = "addq";
			I_ORQ =>  name = "orq";
			I_ADCQ => name = "adcq";
			I_ANDQ => name = "andq";
			I_SUBQ => name = "subq";
			I_CMPQ => name = "cmpq";
			I_MULQ => name = "imulq";
			I_DIVQ => name = "divq";
			I_IDIVQ => name = "idivq";
			I_NEGQ => name = "negq";
			I_NOTQ => name = "notq";
			I_LEAQ => name = "leaq";
			I_TESTQ => name = "testq";
			I_CQO => name = "cqo";
			I_MOVD => name = "movd";
			I_MOVQ => name = "movq";
			I_MOVSS => name = "movss";
			I_MOVSD => name = "movsd";
			I_CVTSS2SD => name = "cvtss2sd";
			I_CVTSD2SS => name = "cvtsd2ss";
			I_CVTSS2SID, I_CVTSS2SIQ => name = "cvtss2si";
			I_CVTSD2SID, I_CVTSD2SIQ => name = "cvtsd2si";
			I_CVTSI2SSD, I_CVTSI2SSQ => name = "cvtsi2ss";
			I_CVTSI2SDD, I_CVTSI2SDQ => name = "cvtsi2sd";
			I_PCMPEQD => name = "pcmpeqd";
			I_PCMPEQQ => name = "pcmpeqq";
			I_FLDD => name = "fldd";
			I_FLDQ => name = "fldq";
			I_UCOMISS => name = "ucomiss";
			I_UCOMISD => name = "ucomisd";
			I_PSLLD => name = "pslld";
			I_PSRLD => name = "psrld";
			I_PSLLQ => name = "psllq";
			I_PSRLQ => name = "psrlq";
			I_ROUNDSS => {
				name = "roundss";
				rounding = true;
			}
			I_ROUNDSD => {
				name = "roundsd";
				rounding = true;
			}
			I_MOVB => name = "movb";
			I_MOVBSX => name = "movbsx";
			I_MOVBZX => name = "movbzx";
			I_MOVW => name = "movw";
			I_MOVWSX => name = "movwsx";
			I_MOVWZX => name = "movwzx";
			I_CMPXCHG8 => name = "cmpxchgb";
			I_CMPXCHG16 => name = "cmpxchgw";
			I_CMPXCHG32 => name = "cmpxchgd";
			I_CMPXCHG64 => name = "cmpxchgq";
			I_INCQ => name = "incq";
			I_DECQ => name = "decq";
			I_SHLQ => name = "shlq";
			I_SARQ => name = "sarq";
			I_SHRQ => name = "shrq";
			I_SHLD => name = "shld";
			I_SARD => name = "sard";
			I_SHRD => name = "shrd";
			I_CALL => name = "call";
			I_ADDSS => name = "addss";
			I_SUBSS => name = "subss";
			I_MULSS => name = "mulss";
			I_DIVSS => name = "divss";
			I_SQRTSS => name = "sqrtss";
			I_ADDSD => name = "addsd";
			I_SUBSD => name = "subsd";
			I_MULSD => name = "mulsd";
			I_DIVSD => name = "divsd";
			I_SQRTSD => name = "sqrtsd";
			I_CALLER_IP => name = "caller_ip";
			I_CALLER_SP => name = "caller_sp";
			I_JMP => name = "j";
			I_SETC => {
				name = "set";
				cond = x86codegen.decodeCond(opcode);
			}
			I_JC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			ArchInstrs.ARCH_RET => {
				putIndent(indent);
				puts(name);
				sp();
				if (codegen.frame.frameSize < 0) puts("?");
				else putd(x86codegen.frameAdjust());
				sp();
				putOperands(a);
				return indent;
			}
			I_THROW => {
				name = "j";
			}
			I_THROWC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			I_TEST_ALLOC => {
				putIndent(indent);
				puts("xaddq [").green().puts("CiRuntime.heapCurLoc").end().puts("], ");
				putOperand(a[0]);
				return indent;
			}
			_ => {
				return putSimpleInstr(indent, i);
			}
		}
		match ((opcode >> AM_SHIFT & 0x1F)) {
			AM_MRRSD_XMM,
			AM_MRRSD_REG,
			AM_MRRSD_IMM => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				var offset = renderMrrsd(a, 0);
				csp();
				putOperand(a[offset]);
			}
			AM_XMM_MRRSD,
			AM_REG_MRRSD => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperand(a[0]);
				csp();
				var offset = renderMrrsd(a, 1);
			}
			_ => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperands(a);
			}
		}
		if (rounding) {
			var r = x86codegen.decodeRounding(opcode);
			sp();
			puts(r.name);
		}
		return indent;
	}
	def renderMrrsd(a: Array<Operand>, start: int) -> int {
		var x = a[start], y = a[start + 1], scale = codegen.toInt(a[start + 2]), disp = codegen.toImm(a[start + 3]);
		putc('[');
		var prev = false;
		if (Operand.Use.?(x)) {
			putOperand(x);
			prev = true;
		}
		if (Operand.Use.?(y)) {
			if (prev) puts(" + ");
			putOperand(y);
			prev = true;
		}
		if (scale > 1) {
			puts(" * ");
			putd(scale);
		}
		if (disp != null) {
			if (prev) puts(" + ");
			green().putv(disp, null).end();
		}
		putc(']');
		return start + 4;
	}
}
// Pattern match of a comparison between two vars or a var and a constant
class X86_64CmpMatch {
	def size: byte;
	def cond: X86_64Cond;
	def x: SsaInstr;
	def y: SsaInstr;
	new(size, cond, x, y) { }
	def negate() -> X86_64CmpMatch { return X86_64CmpMatch.new(size, cond.negate, x, y); }
}
