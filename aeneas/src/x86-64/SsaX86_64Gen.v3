// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def I_ADDD	= 0x01;		def I_ADDQ	= 0x11;
def I_ORD	= 0x02;		def I_ORQ	= 0x12;
def I_ADCD	= 0x03;		def I_ADCQ	= 0x13;
def I_ANDD	= 0x04;		def I_ANDQ	= 0x14;
def I_SUBD	= 0x05;		def I_SUBQ	= 0x15;
def I_XORD	= 0x06;		def I_XORQ	= 0x16;
def I_CMPD	= 0x07;		def I_CMPQ	= 0x17;
def I_MULD	= 0x08;		def I_MULQ	= 0x18;
def I_NEGD	= 0x09;		def I_NEGQ	= 0x19;
def I_NOTD	= 0x0A;		def I_NOTQ	= 0x1A;
def I_TESTD	= 0x0B;		def I_TESTQ	= 0x1B;
def I_LEAD	= 0x0C;		def I_LEAQ	= 0x1C;
def I_DIVD	= 0x0D;		def I_DIVQ	= 0x1D;
def I_IDIVD	= 0x0E;		def I_IDIVQ	= 0x1E;
def I_INCD	= 0x0F;		def I_INCQ	= 0x1F;

def I_DECD	= 0x20;		def I_DECQ	= 0x30;
def I_SHLD	= 0x21;		def I_SHLQ	= 0x31;
def I_SARD	= 0x22;		def I_SARQ	= 0x32;
def I_SHRD	= 0x23;		def I_SHRQ	= 0x33;
def I_CDQ	= 0x24;		def I_CQO	= 0x34;

def I_JMP	= 0x25;
def I_JC	= 0x26;
def I_SETC	= 0x27;
def I_THROW	= 0x28;
def I_THROWC	= 0x29;
def I_SWITCH	= 0x2A;
def I_MOVD	= 0x2B;
def I_MOVQ	= 0x2C;
def I_LD8	= 0x2D;
def I_LDS8	= 0x2E;
def I_LDU8	= 0x2F;

def I_LD16		= 0x35;
def I_LDS16		= 0x36;
def I_LDU16		= 0x37;
def I_LD32		= 0x38;
def I_LD64		= 0x39;
def I_ST8		= 0x3A;
def I_ST16		= 0x3B;
def I_ST32		= 0x3C;
def I_ST64		= 0x3D;
def I_CMPXCHG8		= 0x3E;
def I_CMPXCHG16		= 0x3F;

def I_CMPXCHG32		= 0x40;
def I_CMPXCHG64		= 0x41;
def I_CALL		= 0x42;
def I_CALLER_IP		= 0x43;
def I_CALLER_SP		= 0x44;
def I_TEST_ALLOC	= 0x45;

def I_QD_DIFF = I_ADDQ - I_ADDD; // Used to compute 64-bit opcode from 32-bit opcode

// Addressing modes:
// REG = register, OP = register/stack, IMM = immediate
// MRRSD = [reg + reg * scale + disp]
def AM_NONE	 = 0x00;
def AM_REG_OP	 = 0x01;
def AM_ADDR_IMM  = 0x02;
def AM_ADDR_REG  = 0x03;
def AM_MRRSD_REG = 0x04;
def AM_MRRSD_IMM = 0x05;
def AM_REG_MRRSD = 0x06;
def AM_OP	 = 0x07;
def AM_OP_IMM	 = 0x08;
def AM_OP_REG	 = 0x09;

def AM_SHIFT = u5.view(8);
def COND_SHIFT = u5.view(12);

def ABS_MARKER = 0x11223344;
def REL_MARKER = 0xAABBCCDD;

def MATCH_NEG = true;
def MATCH_OP_I = true;
def MATCH_ADDR_ADD = true;
def MATCH_ADDR_ADD_IMM = true;
def MATCH_ADDR_SCALE = true;
def MATCH_SCALE = true;

def Regs: X86_64RegSet;
def Conds: X86_64Conds;

// Generates X86-64 machine code from SSA.
class SsaX86_64Gen extends SsaMachGen {
	def asm: X86_64MacroAssembler;
	def m = SsaInstrMatcher.new();
	var labelUses: List<(int, Label)>;

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter) super(context, mach, Regs.SET, w) {
		anyReg = Regs.GPR_CLASS;
	}

	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => emitSimpleBinop(selectWidth(i, I_ADDD), i, m.intbinop(i));
			IntSub => {
				var yval = m.intbinop(i);
				if (MATCH_NEG && m.xconst && m.xint == 0) return emit1(I_NEGQ | (AM_OP << AM_SHIFT), ovwReg(i, m.y));
				emitSimpleBinop(selectWidth(i, I_SUBD), i, yval);
			}
			IntMul => {
				// XXX: multiply by 1, 2, 3, 4, 5, 9, powers of 2
				var opcode = selectWidth(i, I_MULD);
				var yval = m.intbinop(i);
				if (MATCH_OP_I && m.yconst) return emit2(opcode | (AM_OP_IMM << AM_SHIFT), ovwReg(i, m.x), useInt(yval));
				return emit2(opcode | (AM_REG_OP << AM_SHIFT), ovwReg(i, m.x), use(m.y));
			}
			IntDiv => visitIntDivMod(i, false);
			IntMod => visitIntDivMod(i, true);
			BoolAnd => emitSimpleBinop(I_ANDD, i, m.boolbinop(i));
			BoolOr => emitSimpleBinop(I_ORD, i, m.boolbinop(i));
			IntAnd => emitSimpleBinop(selectWidth(i, I_ANDD), i, m.intbinop(i));
			IntOr => emitSimpleBinop(selectWidth(i, I_ORD), i, m.intbinop(i));
			IntXor => emitSimpleBinop(selectWidth(i, I_XORD), i, m.intbinop(i));
			IntShl => emitShift(i, selectWidth(i, I_SHLD));
			IntSar => emitShift(i, selectWidth(i, I_SARD));
			IntShr => emitShift(i, selectWidth(i, I_SHRD));
			IntViewI => {
				var tt = IntType.!(i.op.typeArgs[1]);
				if (tt.width == 64) {
					// XXX: make sure nop int converts are removed by lowering
					emit2(I_MOVQ | (AM_REG_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
				} else if (!tt.signed) {
					emit2(I_ANDQ | (AM_OP_IMM << AM_SHIFT), ovwReg(i, i.input0()), useImm(tt.max));
				} else {
					var t = newTmp(tt);
					emit2(I_SHLQ | (AM_OP_IMM << AM_SHIFT), ovwReg(t, i.input0()), useInt(64 - tt.width));
					emit2(I_SARQ | (AM_OP_IMM << AM_SHIFT), ovwReg(i, t), useInt(64 - tt.width));
				}
			}
			BoolEq,
			BoolNot,
			RefEq,
			IntEq,
			IntLt,
			IntLteq,
			PtrLt,
			PtrLteq => {
				var cmp = matchCmp(i, false);
				emitCmp(cmp);
				// TODO: zero-extend?
				emit1(ArchInstrs.FLAG_NO_GAP | I_SETC | encodeCond(cmp.cond), dfnFixed(i, Regs.GPR_CLASS));
			}
			ConditionalThrow(exception) => {
				var cmp = matchCmp(i.input0(), true);
				emitCmp(cmp);
				emit1(ArchInstrs.FLAG_NO_GAP | I_THROWC | encodeCond(cmp.cond), useExSource(exception, i.source));
			}
			PtrLoad => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					return emitN(I_TESTQ | (AM_MRRSD_IMM << AM_SHIFT));
				}
				var opcode: int, signed = V3.isSigned(lt);
				// TODO: floating point loads
				match (size) {
					1 => opcode = if(signed, I_LDS8, I_LDU8);
					2 => opcode = if(signed, I_LDS16, I_LDU16);
					4 => opcode = I_LD32;
					8 => opcode = I_LD64;
					_ => context.fail("invalid size for load");
				}
				dfnReg(i);
				useMrrsd(t);
				// TODO: source position for NullCheckException
				return emitN(opcode | (AM_REG_MRRSD << AM_SHIFT));
			}
			PtrStore => {
				var lt = i.op.typeArgs[0], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					return emitN(I_TESTQ | (AM_MRRSD_IMM << AM_SHIFT));
				}
				var opcode: int;
				useMrrsd(t);
				useReg(i.input1()); // TODO: floating point stores
				match (size) {
					1 => opcode = I_ST8;
					2 => opcode = I_ST16;
					4 => opcode = I_ST32;
					8 => opcode = I_ST64;
					_ => context.fail("invalid size for store");
				}
				// TODO: source position for NullCheckException
				return emitN(opcode | (AM_MRRSD_REG << AM_SHIFT));
			}
			PtrCmpSwp => {
				var lt = i.op.typeArgs[0], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				var opcode: int;
				useMrrsd(t);
				var expect = useFixed(i.input1(), Regs.RAX);
				useFixed(i.input1(), Regs.NOT_RAX);
				match (size) {
					1 => opcode = I_CMPXCHG8;
					2 => opcode = I_CMPXCHG16;
					4 => opcode = I_CMPXCHG32;
					8 => opcode = I_CMPXCHG64;
					_ => context.fail("invalid size for cmpswp");
				}
				// TODO: source position for NullCheckException
				emitN(opcode | (AM_MRRSD_REG << AM_SHIFT)); // TODO: addressing mode is wrong
				emit1(ArchInstrs.FLAG_NO_GAP | I_SETC | encodeCond(X86_64Conds.Z), dfnReg(i));
			}
			PtrAdd => {
				// XXX: use binop matching for PtrAdd
				emit2(I_ADDQ | (AM_REG_OP << AM_SHIFT), ovwReg(i, i.input0()), use(i.input1()));
			}
			PtrSub => {
				// XXX: use binop matching for PtrAdd
				emit2(I_SUBQ | (AM_REG_OP << AM_SHIFT), ovwReg(i, i.input0()), use(i.input1()));
			}
			Alloc => {
				// TODO: hardwired to test allocation routine
				emit1(I_TEST_ALLOC | (AM_OP << AM_SHIFT), ovwReg(i, i.input0()));
			}
			CallAddress(funcRep) => visitCall(i, funcRep);
			CallerIp => {
				emit1(I_CALLER_IP, dfnReg(i));
			}
			CallerSp => {
				emit1(I_CALLER_SP, dfnReg(i));
			}
			CallKernel => ; // TODO
			TupleGetElem => ; // do nothing; calls will define their projections
			_ => return context.fail("unexpected opcode");
		}
	}
	def visitIntDivMod(i: SsaApplyOp, isMod: bool) {
		var it = IntType.!(i.op.typeArgs[0]);
		var upper: SsaInstr;
		if (it.signed) {
			upper = newTmp(it);
			var opcode = if(it.width > 32, I_CQO, I_CDQ);
			emit2(opcode | (AM_OP_REG << AM_SHIFT), dfnFixed(upper, Regs.RDX), useFixed(i.input0(), Regs.RAX));
		}
		var d = dfnFixed(i, if(isMod, Regs.RDX, Regs.RAX));
		var k = kill(-1, if(isMod, Regs.RAX, Regs.RDX));
		var a = useFixed(i.input0(), Regs.RAX);
		if (it.signed) useFixed(upper, Regs.RDX);
		else useConstant(context.graph.zeroConst(), Regs.RDX);
		var c = useFixed(i.input1(), Regs.NOT_RAX_RDX);
		// TODO: source position for DivideByZeroException
		return emitN(selectWidth(i, if(it.signed, I_IDIVD, I_DIVD)));
	}
	def selectWidth(i: SsaApplyOp, op: int) -> int {
		return if(isInt64(i), op + I_QD_DIFF, op);
	}
	def isInt64(i: SsaApplyOp) -> bool {
		// XXX: factor this out and clean it up
		var t = i.op.typeArgs[0];
		if (IntType.?(t)) return IntType.!(t).width > 32;
		if (t.typeCon.kind == V3Kind.ENUM) return false;
		if (t.typeCon.kind == V3Kind.ENUM_SET) return V3.getEnumSetType(t).width > 32;
		return true;
	}
	def visitCall(call: SsaApplyOp, funcRep: Mach_FuncRep) {
		var func = call.input0(), mi: MachInstr;
		var conv = frame.allocCallerSpace(X86_64VirgilCallConv.getForFunc(funcRep));

		// define the return value(s) of the call
		var rv = getProjections(call);
		for (i < rv.length) {
			var r = rv[i];
			if (r != null) dfnFixed(r, conv.calleeRet(i));
		}
		// TODO: var lp = if(rtgc != null, livePoint());
		kill(newLivepoint(), Regs.ALL);	// XXX: manually split liveness across call?
		var skip = 0;
		if (SsaConst.?(func)) {
			var target = Address<IrMethod>.!(SsaConst.!(func).val);
			useImm(target);
			if (target == null || target.val == null || V3.isComponent(target.val.receiver)) skip = 1;
		} else {
			useReg(func);
		}

		// use the arguments to the call
		var inputs = call.inputs;
		for (i = 1 + skip; i < inputs.length; i++) {  // input[0] == func
			useFixed(inputs[i].dest, conv.calleeParam(i - 1));
		}
		emitN(I_CALL);
	}
	def emitShift(i: SsaApplyOp, opcode: int) {
		var yval = m.intbinop(i);
		if (MATCH_OP_I && m.yconst) return emit2(opcode | (AM_OP_IMM << AM_SHIFT), ovwReg(i, m.x), useInt(yval));
		return emit2(opcode | (AM_OP << AM_SHIFT), ovwReg(i, m.x), useFixed(m.y, Regs.RCX));
	}
	def useMrrsd(x: SsaInstr, y: SsaInstr, scale: byte, disp: Val) {
		if (x == null) useInt(0); // no base register, will ignore
		else useReg(x);
		useReg(y);
		useInt(scale);
		useImm(disp);
	}
	def matchMrrsd(i: SsaInstr) -> (SsaInstr, SsaInstr, byte, Val) {
		var t = matchAddImm(i, null), i = t.0, disp = t.1;
		var xadd: SsaApplyOp;
		if (MATCH_ADDR_ADD && (xadd = cover(Opcode.PtrAdd.tag, i)) != null) {
			var x = xadd.input0(), y = xadd.input1();
			var xs = matchScale(x);
			if (xs.1 != 1) return (y, xs.0, xs.1, disp);
			var ys = matchScale(y);
			return (x, ys.0, ys.1, disp);
		}
		var is = matchScale(i);
		return (null, is.0, is.1, disp);
	}
	def matchScale(i: SsaInstr) -> (SsaInstr, byte) {
		if (!MATCH_SCALE) return (i, 1);
		if (!SsaApplyOp.?(i)) return (i, 1);
		var apply = SsaApplyOp.!(i);
		if (Opcode.IntMul.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 1) return (m.x, 1);
			if (yval == 2) return (m.x, 2);
			if (yval == 4) return (m.x, 4);
			if (yval == 8) return (m.x, 8);
		} else if (Opcode.IntShl.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 0) return (m.x, 1);
			if (yval == 1) return (m.x, 2);
			if (yval == 2) return (m.x, 4);
			if (yval == 3) return (m.x, 8);
		}
		return (apply, 1);
	}
	def emitSimpleBinop(opcode: int, i: SsaApplyOp, yval: int) {
		// XXX: select better left operand using liveness
		// XXX: cover loads with mrrsd operand
		if (MATCH_OP_I && m.yconst) return emit2(opcode | (AM_OP_IMM << AM_SHIFT), ovwReg(i, m.x), useInt(yval));
		return emit2(opcode | (AM_REG_OP << AM_SHIFT), ovwReg(i, m.x), use(m.y));
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {
		emit1(I_THROW, useExSource(i.exception, i.source));
	}
	def visitIf(block: SsaBlock, i: SsaIf) {
		var key = i.input0(), cmp = matchCmp(key, true), succ = i.block().succ;
		var s0 = succ(0).dest, s1 = succ(1).dest, target: SsaBlock, jmp: SsaBlock;
		if (blocks.isImmediatelyAfter(context.block, s1)) { // fall through to s1
			target = s0;
		} else if (blocks.isImmediatelyAfter(context.block, s0)) {  // fall through to s0
			cmp = cmp.negate();
			target = s1;
		} else {  // cannot fall through
			target = s0;
			jmp = s1;
		}
		emitCmp(cmp);
		emit1(ArchInstrs.FLAG_NO_GAP | I_JC | encodeCond(cmp.cond), useLabel(target));
		if (jmp != null) emit1(ArchInstrs.FLAG_NO_GAP | I_JMP, useLabel(jmp));
	}
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {
		use(i.input0());
		useImm(Int.box(i.minValue));
		useScratch();
		for (s in block.succs()) useLabel(s.dest);
		emitN(I_SWITCH);
	}
	def visitGoto(block: SsaBlock, i: SsaGoto) {
		var target = i.target();
		if (!blocks.isImmediatelyAfter(context.block, target)) {
			emit1(I_JMP, useLabel(target));  // jump to block if not successor
		}
	}

	def genSaveLocal(reg: int, v: VReg) {
		if (Regs.SET.isStack(reg)) return;
		var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ); // TODO: XMM
		emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(v.ssa, v, v.spill)), op(Operand.Use(null, null, reg)));
	}
	def genRestoreLocal(v: VReg, reg: int) {
		var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ); // TODO: XMM
		if (v.isConst()) {
			var val = SsaConst.!(v.ssa).val;
			emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(null, null, reg)), op(Operand.Immediate(val)));
			return;
		}
		if (Regs.SET.isStack(reg)) {
			// stack to stack move
			var scratch = Regs.SCRATCH_GPR; // TODO: XMM
			emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, null, scratch)), op(Operand.Use(v.ssa, v, v.spill)));
			emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(null, null, reg)), op(Operand.Use(null, null, scratch)));
		} else {
			emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, null, reg)), op(Operand.Use(v.ssa, v, v.spill)));
		}
	}

	def emitCmp(cmp: X86_64CmpMatch) {
		var opcode = if(cmp.is64, I_CMPQ, I_CMPD);
		if (cmp.y == null) {
			emit2(opcode | (AM_OP_IMM << AM_SHIFT), use(cmp.x), useImm(cmp.val));
		} else {
			emit2(opcode | (AM_REG_OP << AM_SHIFT), useReg(cmp.x), use(cmp.y));
		}
	}

	def assemble(opcode: int, a: Array<Operand>) {
		if (opcode < 0) {
			match (opcode) {
				ArchInstrs.ARCH_ENTRY => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.subq_r_i(X86_64Regs.RSP, adjust); // allocate frame
				}
				ArchInstrs.ARCH_BLOCK => {
					var label = toLabel(a[0]);
					label.pos = asm.pos();
				}
				ArchInstrs.ARCH_RET => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.addq_r_i(X86_64Regs.RSP, adjust); // deallocate frame
					asm.ret();
				}
			}
			return;
		}
		var start = asm.pos(), addr: Addr;
		var mode = ((opcode >> AM_SHIFT) & 0xF);
		match (mode) {
			AM_NONE => {
				assemble_none(opcode, a);
			}
			AM_OP => {
				var loc = toLoc(a[0]), reg = Regs.toGpr(loc);
				if (reg != null) assemble_r(opcode, reg);
				else assemble_m(opcode, loc_m(loc));
			}
			AM_OP_REG => {
				var loc = toLoc(a[0]), b = toReg(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_r(opcode, a, b);
				else assemble_m_r(opcode, loc_m(loc), b);
			}
			AM_OP_IMM => {
				var loc = toLoc(a[0]), imm = toImm(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_i(opcode, a, imm);
				else assemble_m_i(opcode, loc_m(loc), imm);
			}
			AM_REG_OP => {
				var reg = toReg(a[0]), loc = toLoc(a[1]), b = Regs.toGpr(loc);
				if (b != null) assemble_r_r(opcode, reg, b);
				else assemble_r_m(opcode, reg, loc_m(loc));
			}
			AM_MRRSD_REG => {
				var m = toMrrsd(a, 0);
				assemble_m_r(opcode, m, toReg(a[4]));
			}
			AM_MRRSD_IMM => {
				var m = toMrrsd(a, 0);
				assemble_m_i(opcode, m, toImm(a[3]));
			}
			AM_REG_MRRSD => {
				var m = toMrrsd(a, 1);
				assemble_r_m(opcode, toReg(a[0]), m);
			}
			_ => return context.fail1("unknown addressing mode %d", mode);
		}
	}
	def assemble_r(opcode: int, a: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.negd_r(a);
			I_NOTD => asm.notd_r(a);
			I_MULD => asm.imuld_r(a);
			I_INCD => asm.incd_r(a);
			I_DECD => asm.decd_r(a);
			I_SHLD => asm.shld_r_cl(a);
			I_SARD => asm.sard_r_cl(a);
			I_SHRD => asm.shrd_r_cl(a);
			I_DIVD => asm.divd_r(a);
			I_IDIVD => asm.idivd_r(a);

			I_NEGQ => asm.negq_r(a);
			I_NOTQ => asm.notq_r(a);
			I_MULQ => asm.imulq_r(a);
			I_INCQ => asm.incq_r(a);
			I_DECQ => asm.decq_r(a);
			I_SHLQ => asm.shlq_r_cl(a);
			I_SARQ => asm.sarq_r_cl(a);
			I_SHRQ => asm.shrq_r_cl(a);
			I_DIVQ => asm.divq_r(a);
			I_IDIVQ => asm.idivq_r(a);
			I_TEST_ALLOC => {
				var addr = X86_64AddrRef.new(null, null, 1, CiRuntimeModule.HEAP_CUR_LOC);
				asm.xaddq_m_r(addr, a);
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m(opcode: int, a: X86_64Addr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.negd_m(a);
			I_NOTD => asm.notd_m(a);
			I_MULD => asm.imuld_m(a);
			I_INCD => asm.incd_m(a);
			I_DECD => asm.decd_m(a);
			I_SHLD => asm.shld_m_cl(a);
			I_SARD => asm.sard_m_cl(a);
			I_SHRD => asm.shrd_m_cl(a);
			I_DIVD => asm.divq_m(a); // TODO
			I_IDIVD => asm.idivq_m(a); // TODO

			I_NEGQ => asm.negq_m(a);
			I_NOTQ => asm.notq_m(a);
			I_MULQ => asm.imulq_m(a);
			I_INCQ => asm.incq_m(a);
			I_DECQ => asm.decq_m(a);
			I_SHLQ => asm.shlq_m_cl(a);
			I_SARQ => asm.sarq_m_cl(a);
			I_SHRQ => asm.shrq_m_cl(a);
			I_DIVQ => asm.divq_m(a);
			I_IDIVQ => asm.idivq_m(a);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_r_r(opcode: int, a: X86_64Gpr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_r(a, b);
			I_ORD => asm.ord_r_r(a, b);
			I_ADCD => asm.adcd_r_r(a, b);
			I_ANDD => asm.andd_r_r(a, b);
			I_SUBD => asm.subd_r_r(a, b);
			I_XORD => asm.xord_r_r(a, b);
			I_CMPD => asm.cmpd_r_r(a, b);
			I_MULD => asm.imuld_r_r(a, b);
			I_CDQ => asm.cdq();

			I_ADDQ => asm.addq_r_r(a, b);
			I_ORQ => asm.orq_r_r(a, b);
			I_ADCQ => asm.adcq_r_r(a, b);
			I_ANDQ => asm.andq_r_r(a, b);
			I_SUBQ => asm.subq_r_r(a, b);
			I_XORQ => asm.xorq_r_r(a, b);
			I_CMPQ => asm.cmpq_r_r(a, b);
			I_MULQ => asm.imulq_r_r(a, b);
			I_CQO => asm.cqo();

			I_MOVD => asm.movd_r_r(a, b);
			I_MOVQ => asm.movq_r_r(a, b);
			I_LD8 => asm.movb_r_r(a, b);
			I_LDS8 => asm.movbsx_r_r(a, b);
			I_LDU8 => asm.movbzx_r_r(a, b);
			I_LDS16 => asm.movwsx_r_r(a, b);
			I_LDU16 => asm.movwzx_r_r(a, b);
			I_LD32 => asm.movd_r_r(a, b);
			I_LD64 => asm.movq_r_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_r(opcode: int, a: X86_64Addr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_m_r(a, b);
			I_ORD =>  asm.ord_m_r(a, b);
			I_ADCD => asm.adcd_m_r(a, b);
			I_ANDD => asm.andd_m_r(a, b);
			I_SUBD => asm.subd_m_r(a, b);
			I_XORD => asm.xord_m_r(a, b);
			I_CMPD => asm.cmpd_m_r(a, b);
			I_TESTD => asm.testd_m_r(a, b);

			I_ADDQ => asm.addq_m_r(a, b);
			I_ORQ =>  asm.orq_m_r(a, b);
			I_ADCQ => asm.adcq_m_r(a, b);
			I_ANDQ => asm.andq_m_r(a, b);
			I_SUBQ => asm.subq_m_r(a, b);
			I_XORQ => asm.xorq_m_r(a, b);
			I_CMPQ => asm.cmpq_m_r(a, b);
			I_TESTQ => asm.testq_m_r(a, b);

			I_MOVD => asm.movd_m_r(a, b);
			I_MOVQ => asm.movq_m_r(a, b);
			I_ST8 => asm.movb_m_r(a, b);
			I_ST16 => asm.movw_m_r(a, b);
			I_ST32 => asm.movd_m_r(a, b);
			I_ST64 => asm.movq_m_r(a, b);
			I_CMPXCHG8 => asm.cmpxchgb_m_r(a, b);
			I_CMPXCHG16 => asm.cmpxchgw_m_r(a, b);
			I_CMPXCHG32 => asm.cmpxchgd_m_r(a, b);
			I_CMPXCHG64 => asm.cmpxchgq_m_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_r_m(opcode: int, a: X86_64Gpr, b: X86_64Addr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_m(a, b);
			I_ORD =>  asm.ord_r_m(a, b);
			I_ADCD => asm.adcd_r_m(a, b);
			I_ANDD => asm.andd_r_m(a, b);
			I_SUBD => asm.subd_r_m(a, b);
			I_XORD => asm.xord_r_m(a, b);
			I_CMPD => asm.cmpd_r_m(a, b);
			I_TESTD => asm.testq_r_m(a, b);

			I_ADDQ => asm.addq_r_m(a, b);
			I_ORQ =>  asm.orq_r_m(a, b);
			I_ADCQ => asm.adcq_r_m(a, b);
			I_ANDQ => asm.andq_r_m(a, b);
			I_SUBQ => asm.subq_r_m(a, b);
			I_XORQ => asm.xorq_r_m(a, b);
			I_CMPQ => asm.cmpq_r_m(a, b);
			I_TESTQ => asm.testq_r_m(a, b);

			I_MOVD => asm.movd_r_m(a, b);
			I_MOVQ => asm.movq_r_m(a, b);
			I_ST8 => asm.movb_r_m(a, b);
			I_LD8 => asm.movb_r_m(a, b);
			I_LDS8 => asm.movbsx_r_m(a, b);
			I_LDU8 => asm.movbzx_r_m(a, b);
			I_LD16 => asm.movw_r_m(a, b);
			I_LDS16 => asm.movwsx_r_m(a, b);
			I_LDU16 => asm.movwzx_r_m(a, b);
			I_LD32 => asm.movd_r_m(a, b);
			I_LD64 => asm.movq_r_m(a, b);
//TODO			I_CMPXCHG8 => asm.cmpxchgb_r_m(a, b);
//TODO			I_CMPXCHG16 => asm.cmpxchgw_r_m(a, b);
//TODO			I_CMPXCHG32 => asm.cmpxchgd_r_m(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_i(opcode: int, a: X86_64Addr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_m_i(a, b);
			I_ORD =>  asm.ord_m_i(a, b);
			I_ADCD => asm.adcd_m_i(a, b);
			I_ANDD => asm.andd_m_i(a, b);
			I_SUBD => asm.subd_m_i(a, b);
			I_XORD => asm.xord_m_i(a, b);
			I_CMPD => asm.cmpd_m_i(a, b);
			I_TESTD => asm.testd_m_i(a, b);

			I_ADDQ => asm.addq_m_i(a, b);
			I_ORQ =>  asm.orq_m_i(a, b);
			I_ADCQ => asm.adcq_m_i(a, b);
			I_ANDQ => asm.andq_m_i(a, b);
			I_SUBQ => asm.subq_m_i(a, b);
			I_XORQ => asm.xorq_m_i(a, b);
			I_CMPQ => asm.cmpq_m_i(a, b);
			I_MOVD => asm.movd_m_i(a, b);
			I_MOVQ => asm.movq_m_i(a, b);
			I_TESTQ => asm.testq_m_i(a, b);
			I_ST8 => asm.movb_m_i(a, b);
			I_ST16 => asm.movw_m_i(a, b);
			I_ST32 => asm.movd_m_i(a, b);
//TODO			I_CMPXCHG8 => asm.cmpxchgb_m_i(a, b);
//TODO			I_CMPXCHG16 => asm.cmpxchgw_m_i(a, b);
//TODO			I_CMPXCHG32 => asm.cmpxchgd_m_i(a, b);
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_r_i(opcode: int, a: X86_64Gpr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.addd_r_i(a, b);
			I_ORD =>  asm.ord_r_i(a, b);
			I_ADCD => asm.adcd_r_i(a, b);
			I_ANDD => asm.andd_r_i(a, b);
			I_SUBD => asm.subd_r_i(a, b);
			I_XORD => asm.xord_r_i(a, b);
			I_CMPD => asm.cmpd_r_i(a, b);
			I_MULD => asm.imuld_r_i(a, b);
			I_SHLD => asm.shld_r_i(a, u6.view(b));
			I_SARD => asm.sard_r_i(a, u6.view(b));
			I_SHRD => asm.shrd_r_i(a, u6.view(b));

			I_ADDQ => asm.addq_r_i(a, b);
			I_ORQ =>  asm.orq_r_i(a, b);
			I_ADCQ => asm.adcq_r_i(a, b);
			I_ANDQ => asm.andq_r_i(a, b);
			I_SUBQ => asm.subq_r_i(a, b);
			I_XORQ => asm.xorq_r_i(a, b);
			I_CMPQ => asm.cmpq_r_i(a, b);
			I_MULQ => asm.imulq_r_i(a, b);
			I_MOVD => asm.movd_r_i(a, b);
			I_MOVQ => asm.movq_r_i(a, b);
			I_SHLQ => asm.shlq_r_i(a, u6.view(b));
			I_SARQ => asm.sarq_r_i(a, u6.view(b));
			I_SHRQ => asm.shrq_r_i(a, u6.view(b));
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_none(opcode: int, a: Array<Operand>) {
		match (opcode & 0xFF) {
			I_DIVQ, I_IDIVQ, I_DIVD, I_IDIVD => {
				// TODO: add trap location
				var loc = toLoc(a[4]), reg = Regs.toGpr(loc);
				if (reg != null) assemble_r(opcode, reg);
				else assemble_m(opcode, loc_m(loc));
			}
			I_JMP => {
				var label = toLabel(a[0]);
				if (label.pos >= 0) asm.jmp_rel(label.pos - asm.pos());
				else asm.jmp_rel(REL_MARKER).recordRelLabel32(asm.pos() - 4, label); // XXX: make automatic
			}
			I_JC => {
				var label = toLabel(a[0]), cond = decodeCond(opcode);
				if (label.pos >= 0) asm.jc_rel(cond, label.pos - asm.pos());
				else asm.jc_rel(cond, REL_MARKER).recordRelLabel32(asm.pos() - 4, label); // XXX: make automatic
			}
			I_SETC => {
				var reg = toReg(a[0]), cond = decodeCond(opcode);
				asm.set_r(cond, reg);
				asm.movbzx_r_r(reg, reg); // XXX: remove zero extend for setc?
			}
			I_THROW => {
				var exSource = toExSource(a[0]);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jmp_rel_addr(X86_64AddrRef.new(null, null, 1, addr));
			}
			I_THROWC => {
				var exSource = toExSource(a[0]), cond = decodeCond(opcode);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jc_rel_addr(cond, X86_64AddrRef.new(null, null, 1, addr));
			}
			I_NEGQ => {
				asm.negq_r(toReg(a[0]));
			}
			I_SWITCH => {
				var size = a.length - 4;
				var key = toReg(a[0]), minValue = Int.unbox(toImm(a[1])), scratchReg = X86_64Regs.RBP; // TODO: scratch reg
				asm.movq_r_r(scratchReg, key);
				if (minValue != 0) asm.subq_r_i(scratchReg, minValue);
				asm.cmpq_r_i(scratchReg, size - 1);
				asm.jc_rel(X86_64Conds.A, REL_MARKER).recordRelLabel32(asm.pos() - 4, toLabel(a[a.length - 1])); // XXX: make automatic
				// load from the jump table to follow
				var jtAddr = Addr.new(mach.codeRegion, null, 0);
				asm.movd_r_m(scratchReg, X86_64AddrRef.new(null, scratchReg, 4, jtAddr));
				asm.ijmp_r(scratchReg);
				// align and emit the jump table
				w.align(4);
				jtAddr.absolute = w.endAddr();
				// emit (32-bit) jump table
				for (i = 3; i < a.length; i++) {
					w.zeroN(4);
					asm.recordAbsLabel32(asm.pos() - 4, toLabel(a[i]));
				}
			}
			I_CALL => {
				for (o in a) {
					match (o) {
						Immediate(val) => {
							asm.callr_addr(X86_64AddrRef.new(null, null, 0, Addr.!(val)));
							return;
						}
						Use(ssa, vreg, assignment) => {
							asm.icall_r(loc_r(assignment));
							return;
						}
						_ => ;
					}
				}
				context.fail("no target for call");
			}
			I_CALLER_IP => {
				asm.movd_r_m(toReg(a[0]), X86_64Regs.RSP.plus(frameAdjust()));
			}
			I_CALLER_SP => {
				asm.leaq(toReg(a[0]), X86_64Regs.RSP.plus(frame.size()));
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def toB32(val: Val) -> int {
		var addr: Addr, b: int;
		match (val) {
			x: Box<int> 	=> b = x.val;
			x: Addr 	=> b = ABS_MARKER;
			x: Box<bool> 	=> b = if(x.val, 1);
			null 		=> b = 0;
			_ => ; // TODO
		}
		return b;
	}
	def recordAddr(val: Val) {
		if (Addr.?(val)) {
			// record an absolute patch
			asm.recordAbs32(w.pos - 4, X86_64AddrRef.new(null, null, 0, Addr.!(val)));
		}
	}
	def invalidOpcode(opcode: int) {
		var cond = byte.view(0xF & (opcode >> COND_SHIFT));
		var am = byte.view(0xF & (opcode >> AM_SHIFT));
		var code = byte.view(opcode);
		context.fail(Strings.format3("invalid opcode cond=%x am=%x opcode=%x", cond, am, code));
	}
	def toMrrsd(a: Array<Operand>, start: int) -> X86_64Addr {
		var base: X86_64Gpr, o = a[start + 0];
		if (Operand.Use.?(o)) base = toReg(o);
		var index = toReg(a[start + 1]), scale = byte.view(toInt(a[start + 2]));
		var val = toImm(a[start + 3]);
		if (Addr.?(val)) return X86_64AddrRef.new(base, index, scale, Addr.!(val));
		else return X86_64Addr.new(base, index, scale, Int.unbox(val));
	}

	def matchCmp(i: SsaInstr, inblock: bool) -> X86_64CmpMatch {
		if ((inblock && !inSameBlock(i)) || !SsaApplyOp.?(i)) return X86_64CmpMatch.new(false, Conds.NZ, i, null, null);
		var apply = SsaApplyOp.!(i);
		match (apply.op.opcode) {
			BoolEq => 	return newCmp(false, i, Conds.Z);
			RefEq =>	return newCmp(mach.refSize == 64, i, Conds.Z);
			IntEq => 	return newCmp(isInt64(apply), i, Conds.Z);
			IntLt =>	return newCmp(isInt64(apply), i, signedCmp(i, Conds.L, Conds.C));
			IntLteq =>	return newCmp(isInt64(apply), i, signedCmp(i, Conds.LE, Conds.NA));
			PtrLt =>	return newCmp(true, i, Conds.C);
			PtrLteq =>	return newCmp(true, i, Conds.NA);
			BoolNot => 	return matchCmp(i.input0(), inblock).negate();
			_ => ;
		}
		return X86_64CmpMatch.new(false, Conds.NZ, i, null, null);
	}
	def signedCmp(i: SsaInstr, signed: X86_64Cond, unsigned: X86_64Cond) -> X86_64Cond {
		return if(V3.isSigned(SsaApplyOp.!(i).op.sig.paramTypes[0]), signed, unsigned);
	}
	def newCmp(is64: bool, i: SsaInstr, cond: X86_64Cond) -> X86_64CmpMatch {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(y)) return X86_64CmpMatch.new(is64, cond, x, null, SsaConst.!(y).val);
		if (SsaConst.?(x) && cond.commute != null) return X86_64CmpMatch.new(is64, cond.commute, y, null, SsaConst.!(x).val);
		return X86_64CmpMatch.new(is64, cond, x, y, null);
	}

	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}
	def toReg(o: Operand) -> X86_64Gpr {
		match (o) {
			Overwrite(i, dst, src, assignment) => return loc_r(assignment);
			Def(i, vreg, assignment) => return loc_r(assignment);
			Use(i, vreg, assignment) => return loc_r(assignment);
			_ => return V3.fail("expected register");
		}
	}
	def toLoc(o: Operand) -> int{
		match (o) {
			Overwrite(i, dst, src, assignment) => return assignment;
			Def(i, vreg, assignment) => return assignment;
			Use(i, vreg, assignment) => return assignment;
			_ => return V3.fail("expected operand with assignment");
		}
	}

	def encodeCond(cond: X86_64Cond) -> int {
		return cond.index << COND_SHIFT;
	}
	def decodeCond(opcode: int) -> X86_64Cond {
		return Conds.all[(opcode >> COND_SHIFT) & 0xF];
	}

	def loc_r(loc: int) -> X86_64Gpr {
		var gpr = Regs.toGpr(loc);
		if (gpr == null) return V3.fail1("expected GPR, got %s", Regs.SET.identify(loc));
		return gpr;
	}
	def loc_m(loc: int) -> X86_64Addr {
		var regSet = frame.conv.regSet, wordSize = mach.data.addressSize, offset = 0;
		if (loc >= regSet.calleeStart) {
			offset = wordSize * (loc - regSet.calleeStart);
		} else if (loc >= regSet.callerStart) {
			offset = frame.size() + (wordSize * (loc - regSet.callerStart));
		} else if (loc >= regSet.spillStart) {
			offset = wordSize * (loc - regSet.spillStart + frame.spillArgs);
		} else {
			return V3.fail1("invalid spill location %s", Regs.SET.identify(loc));
		}
		return X86_64Regs.RSP.plus(offset);
	}
	def getOutput() -> ArchInstrBuffer {
		if (out != null) return out;
		return out = X86InstrBuffer.new(this, context.prog, regSet);
	}
}
class X86InstrBuffer extends ArchInstrBuffer {
	def x86codegen: SsaX86_64Gen;
	new(x86codegen, prog: Program, regSet: MachRegSet) super(x86codegen, prog, regSet) { }
	def putArchInstr(indent: int, i: ArchInstr) -> int {
		var opcode = int.view(i.opcode()), a = i.operands;
		var name: string, cond: X86_64Cond;
		match (opcode & 0xFF) {
			I_SWITCH => name = "switch";
			I_ADDD => name = "addd";
			I_ORD =>  name = "ord";
			I_ADCD => name = "adcd";
			I_ANDD => name = "andd";
			I_SUBD => name = "subd";
			I_CMPD => name = "cmpd";
			I_MULD => name = "imuld";
			I_DIVD => name = "divd";
			I_IDIVD => name = "idivd";
			I_NEGD => name = "negd";
			I_NOTD => name = "notd";
			I_TESTD => name = "testd";
			I_CDQ => name = "cdq";
			I_ADDQ => name = "addq";
			I_ORQ =>  name = "orq";
			I_ADCQ => name = "adcq";
			I_ANDQ => name = "andq";
			I_SUBQ => name = "subq";
			I_CMPQ => name = "cmpq";
			I_MULQ => name = "imulq";
			I_DIVQ => name = "divq";
			I_IDIVQ => name = "idivq";
			I_NEGQ => name = "negq";
			I_NOTQ => name = "notq";
			I_LEAQ => name = "leaq";
			I_TESTQ => name = "testq";
			I_CQO => name = "cqo";
			I_MOVD => name = "movd";
			I_MOVQ => name = "movq";
			I_LD8 => name = "movb";
			I_LDS8 => name = "movbsx";
			I_LDU8 => name = "movbzx";
			I_LD16 => name = "movw";
			I_LDS16 => name = "movwsx";
			I_LDU16 => name = "movwzx";
			I_LD32 => name = "movd";
			I_LD64 => name = "movq";
			I_ST8 => name = "movb";
			I_ST16 => name = "movw";
			I_ST32 => name = "movd";
			I_ST64 => name = "movq";
			I_CMPXCHG8 => name = "cmpxchgb";
			I_CMPXCHG16 => name = "cmpxchgw";
			I_CMPXCHG32 => name = "cmpxchgd";
			I_CMPXCHG64 => name = "cmpxchgq";
			I_INCQ => name = "incq";
			I_DECQ => name = "decq";
			I_SHLQ => name = "shlq";
			I_SARQ => name = "sarq";
			I_SHRQ => name = "shrq";
			I_CALL => name = "call";
			I_CALLER_IP => name = "caller_ip";
			I_CALLER_SP => name = "caller_sp";
			I_JMP => name = "j";
			I_SETC => {
				name = "set";
				cond = x86codegen.decodeCond(opcode);
			}
			I_JC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			ArchInstrs.ARCH_RET => {
				putIndent(indent);
				puts(name);
				sp();
				if (codegen.frame.frameSize < 0) puts("?");
				else putd(x86codegen.frameAdjust());
				sp();
				putOperands(a);
				return indent;
			}
			I_THROW => {
				name = "j";
			}
			I_THROWC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			I_TEST_ALLOC => {
				putIndent(indent);
				puts("xaddq [").green().puts("CiRuntime.heapCurLoc").end().puts("], ");
				putOperand(a[0]);
				return indent;
			}
			_ => {
				return putSimpleInstr(indent, i);
			}
		}
		match ((opcode >> AM_SHIFT & 0xF)) {
			AM_MRRSD_REG,
			AM_MRRSD_IMM => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				var offset = renderMrrsd(a, 0);
				csp();
				putOperand(a[offset]);
			}
			AM_REG_MRRSD => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperand(a[0]);
				csp();
				var offset = renderMrrsd(a, 1);
			}
			_ => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperands(a);
			}
		}

		return indent;
	}
	def renderMrrsd(a: Array<Operand>, start: int) -> int {
		var x = a[start], y = a[start + 1], scale = codegen.toInt(a[start + 2]), disp = codegen.toImm(a[start + 3]);
		putc('[');
		if (!Operand.Immediate.?(x)) {
			putOperand(x);
			puts(" + ");
		}
		putOperand(y);
		if (scale > 1) {
			puts(" * ");
			putd(scale);
		}
		if (disp != null) {
			puts(" + ");
			putv(disp, null);
		}
		putc(']');
		return 3;
	}
}
// Pattern match of a comparison between two vars or a var and a constant
class X86_64CmpMatch {
	def is64: bool;
	def cond: X86_64Cond;
	def x: SsaInstr;
	def y: SsaInstr;
	def val: Val;
	new(is64, cond, x, y, val) { }
	def negate() -> X86_64CmpMatch { return X86_64CmpMatch.new(is64, cond.negate, x, y, val); }
}
